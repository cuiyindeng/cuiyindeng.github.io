[{"title":"java设计模式","date":"2022-07-30T11:14:24.000Z","path":"2022/07/30/java-design-pattern/","text":"设计模式是软件界总结出来的一套可以重复使用的经验，它可以提高代码的可重用性，增强系统的可维护性，以及解决一些列复杂的问题。 开发人员面对需求的变更，设计模式给了我们指导。专家们首先提出了6大设计原则，6个设计原则可以衍生出设计模式的具体实现方法。 1，单一职责原则就一个类而言，应该仅有一个变动它的原因。 2，里式替换原则子类型必须能够替换掉它们的父类型 3，依赖倒置原则1，模块之间的依赖通过抽象发生，实现类之间不产生依赖关系，它们的依赖关系是通过接口或抽象类产生的2，接口或抽象类不依赖实现类3，实现类依赖接口或抽象类 4，接口隔离原则接口尽量细化，同时接口中的的方法尽量少 5，迪米特法则也叫最少知识原则。如果两个类不必直接通讯，那么这两个类不应该发生直接的相互作用。如果一个类需要调用另一个类的某个方法时，可以通过第三者转发这个调用。 6，开闭原则软件实体应该可以扩展，但是不可以修改。 1，单例模式确保某一个类只有一个实例，而且自行实例化并向系统提供这个实例。 2，工厂方法模式定义一个用于创建对象的接口，让接口的子类决定实例化哪一个类。工厂方法使一个类的实例化延迟到其子类。 3，抽象工厂模式为创建每一组相关或相互依赖的对象提供一个接口，而且无需指定他们的具体类。 4，模板方法模式定义一个操作中的算法的框架，而将一些步骤延迟到子类中。使得子类不需要改变一个算法的结构即可重新定义该算法的某些特定步骤。 5，建造者模式将对象的创建和表示分离，使得同样的创建过程可以产生不同的表示。 6，代理模式为其他对象提供一个代理以控制对这个对象的访问。 7，原型模式用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。 8，中介者模式通过一个中介对象封装多个对象之间的交互，中介对象使各对象不需要显式的相互作用，从而使耦合松散，而且可以独立的改变它们之间的交互。 9，命令模式将一个请求封装成一个对象，从而是你可以用不同请求将客户参数化，对请求排队或记录请求日志，可以提供命令的撤销和恢复功能。 10，责任链模式使多个对象都有机会处理请求，从而避免了请求的发送者和接收者的耦合关系。将对象练成一条链，并沿着这条链传递该请求，知道有对象成处理它为止。 11，装饰模式动态的给一个对象添加额外的职责，就增加功能来说，装饰模式比生成子类更加灵活。 12，策略模式定义一组算法，将每个算法都封装起来，并且使它们之间可以互换。 13，适配器模式将一个类的接口转换成客户端所需要的另一种接口，使两个因为接口不匹配而无法在一起工作的两个类能够一起工作。 14，迭代器模式它提供一个方法访问容器对象的每个元素，并且不需要暴露这个对象的内部细节。 15，观察者模式定义对象之间的一种一对多的依赖关系，使得当一个对象改变状态，则所有依赖它的对象都会得到通知并被自动更新。 16，门面模式要求一个子系统的外部和内部的通信必须经过一个统一的对象进行，门面模式提供一个高层次的接口，使得子系统更易于使用。 17，备忘录模式在不破坏封装性的前提下，捕获一个对象的内部状态，并在对象外部保存保存这个状态。这样可以将对象恢复到原先保存的状态。 18，访问者模式封装一些作用于某些数据结构中的各元素的操作，它可以在不改变数据结构的前提下定义作用于这些元素的新的操作。 19，状态模式当一个对象内在状态改变时允许其改变行为，这个对象看起来像改变了其类。 20，解释器模式给定一门语言，定义他的语法的一种表示，并定义一个解释器，该解释器用该表示来解释语言中的句子。 21，享元模式使用共享对象可以有效的支持大量的细粒度的对象。 22，桥梁模式将实现和抽象解耦，使得两者可以独立的变化。 23，组合模式将对象组合成树形结构以表示“部分-整体”的层次结构，使得用户对单个对象和组合对象的使用具有一致性。","tags":[]},{"title":"读《实战java高并发程序设计-葛一鸣》笔记","date":"2021-02-26T23:54:24.000Z","path":"2021/02/27/action-java-concurrency/","text":"java并发的实际运用不管是在工作中还是在面试中，都有着相当重要的作用。所以想写一篇记录自己学习java并发实战的文章。 几个概念 同步和异步：是指一种方法调用，同步的方法调用需要等待方法的结果返回，才能继续后续的操作。异步的的方法调用更像是一个消息传递。方法调用之后，不用等待方法返回结果，调用者就能继续后续的操作。 并发和并行：并发是真正的多个任务同时执行，出现在有多核cpu的系统中；并发是多个任务交替执行，出现在只有一个cpu的系统中。 临界区：表示一种公共资源或者共享数据。可以被多个线程使用，但是同时只能有一个线程使用，当临界区资源被占用，其他线程必须要等待。 阻塞和非阻塞：用来形容多线程间的相互影响，比如一个线程占用了临界区的资源，其他所有需要这个资源的线程都会在临界区中等待，这种情况就是阻塞。非阻塞与其相反，没有一个线程会阻碍其他线程执行。 死锁、饥饿、活锁：死锁是多个线程竞争同一个资源，并且互相不释放占有的资源，导致程序无法执行下去。 并发级别 阻塞：一个线程是阻塞的，那么在其它线程释放资源之前，当前线程是无法继续执行的。使用synchronized或者重入锁，都会得到一个阻塞的线程。 无饥饿：如果锁是不公平的，系统允许高优先级的线程插队，可能会引起低优先级的线程产生饥饿。如果锁是公平的，系统会限制所有将要获取资源的的线程都排队，那么饥饿就不会产生。 无障碍：多个线程可以无障碍的进入同一个资源的临界区，并且不会被挂起，当一个线程检测到资源被另一个线程修改后，会对自己所做的修改进行回滚。无障碍的多线程不一定能顺利执行，因为在临界区中存在严重冲突时，有可能所有的线程会不断的回滚自己的操作，而没有一个线程可以走出临界区，这种情况会影响系统的正常运行。 一种可行的无障碍实现可以依赖一个“一致性标记”来实现。线程在操作之前，需要读取并保存标记的状态。操作完成之后，再次读取，检查该标记是否被更改过。如果更改过，说明资源已经被修改，线程会进行重试操作。如果没更改，说明资源访问没有冲突。 无锁：无锁的并行都是无障碍的。在无锁的状态下，所有的线程都可以尝试对临界区进行访问。与无障碍不同的是，无锁的并发只保证有一个线程进入临界区完成操作并退出。无锁的特点是：会包含一个无限循环。 无等待：","tags":[]},{"title":"java中的集合","date":"2020-02-23T07:15:17.000Z","path":"2020/02/23/java-core-collection/","text":"总结一下java中的集合类型，还有相关的数据结构方面的知识。","tags":[]},{"title":"源码方式安装redis","date":"2019-12-26T07:23:36.000Z","path":"2019/12/26/redis-installation/","text":"在Ubuntu上用源码的方式安装redis 环境准备 123$ sudo apt update$ sudo apt upgrade$ sudo apt install build-essential 下载源码包编译 1234$ sudo wget http://download.redis.io/releases/redis-5.0.7.tar.gz$ sudo tar xzf redis-5.0.7.tar.gz$ sudo cd redis-5.0.7$ sudo make MALLOC=libc ##解决错误：zmalloc.h:50:10: fatal error: jemalloc/jemalloc.h: No such file or directory 拷贝命令到指定目录，用于使用redis_init_script脚本安装并启动redis服务 1234567$ sudo cp src/redis-server /usr/local/bin/$ sudo cp src/redis-cli /usr/local/bin/$ sudo mkdir /etc/redis$ sudo mkdir /var/redis$ sudo mkdir /var/redis/6379$ sudo cp utils/redis_init_script /etc/init.d/redis_6379$ sudo cp redis.conf /etc/redis/6379.conf 修改配置文件，将daemonize项改成yes，dir项改为/var/redis/6379，logfile项改为/var/redis/6379/log/redis.log 1$ sudo vi /etc/redis/6379.conf 设置redis系统服务和启动redis服务 12$ sudo update-rc.d redis_6379 defaults$ sudo /etc/init.d/redis_6379 start ##或者service redis_6379 start 登录redis服务 1234$ redis-cli -h 10.12.9.18 -p 7000 -c -a 12345##ip：10.12.4.45 集群中的一个点##-c 以集群方式登陆。cluster##-a 密码。authority","tags":[]},{"title":"数学复习","date":"2019-12-10T08:08:27.000Z","path":"2019/12/10/advanced-mathmatics/","text":"","tags":[]},{"title":"思考方式","date":"2019-11-15T02:27:35.000Z","path":"2019/11/15/way-of-thinking/","text":"对现代人来说，他的财富并不是每个月收入的数字，而是他有一种什么样的思维方式。 早期的逻辑学并不是一门独立的学科，而是隶属于哲学。 任何思维活动都离不开概念、判断和推理，逻辑论证是用一个已知为真的判断确定另一判断的真实性或虚假性的过程，与此同时，它也是一个综合运用概念、判断和推理的过程。 逻辑论证逻辑论证，是指用已知为真的判断通过逻辑推理确定另一判断真假的思维过程。一般来说，逻辑论证分为论题、论据和论证方式三部分。例如：地球是圆形的（论据），因为任何圆形的物体，从其中某点出发一直向前走（论据），最终都会回到原点；麦哲伦就是从西班牙出发最后又回到了西班牙（论据）。 演绎推理演绎推理的主要形式是“三段论”，由大前提、小前提、结论三部分组成一个“连珠”。 大前提是已知的一般的原理。 小前提是研究的特殊场合。 结论是将特殊场合和归到一般原理之下得出的新知识。例如： 大前提：电流是电子向一定方向形成的。 小前提：金属的自由电子能在电场作用下定向运动。 结论：所以，金属能导电。 归纳推理由一定程度的关于个别事物的观点过渡到范围较大的观点，由特殊具体的事例推导出一般原理、原则的解释方法。 抽象思维","tags":[]},{"title":"学习java8的笔记","date":"2019-11-12T02:35:14.000Z","path":"2019/11/12/java8-notes/","text":"jdk1.8的更新内容，相对于前面的版本来说有了很多重大的改变。 lambda表达式lambda表达式语法12(String first, String second) -&gt; &#123;Integer.compare(first.length(), second.length())&#125; 如果lambda表达式的参数类型是可以被推导出来的，则可以省略参数类型。下面的类型是根据泛型参数推导出来的。12Comparator&lt;String&gt; com = (first, second) -&gt; Integer.compare(first.length(), second.length()); 变量作用域1234567891011121314151617/*** lambda表达式必须存储这两个变量的值。* @param text* @param count*/public static void m(String text, int count) &#123; Runnable r = () -&gt; &#123; /** * lambda表达式被转换为只含有一个方法的对象，两个自由变量的值会被复制到该对象的实例变量中。 * 所以被lambda表达式引用的变量的值是不能被改变的，下面的两句是错误的。 */// count++;// text = \"a\"; System.out.println(text); &#125;; new Thread(r).start();&#125;","tags":[]},{"title":"读《原则》笔记","date":"2019-10-15T05:37:37.000Z","path":"2019/10/15/principle-notes/","text":"因为一段名字叫做《经济是怎么运行的》30分钟的经济知识讲解类视频，从而知道了瑞·达利欧这个人。后来关注了他的微博，了解到他写了一本叫做《原则》的书，可能那个视频就是为他的书做宣传而做的吧。前两天在网上看到了这本书的PDF，所以就想看一下，并记录下来自己读到的一些片段和体会。 第一条原则独立思考并将决定： 你想要什么 事实是什么 面对事实，你怎样实现自己的愿望。 原则是如何总结出来的 找到一种方法，克服顽固难治的问题 找到与我观点不同的聪明人，以便自己能够努力理解他们的推理。 知道自己在什时候不能有明确的意见，不急于下结论。 逐步归纳永恒和普适的原则，对其进行测试，将其系统化。 通过平衡风险保持较大的回报，并降低下行波动。这一经历引导达利欧将桥水打造成一个创意择优的机构；不是专制机构，也不是民主机构。 故事1从1982年底开始，在接下来的18年里，美国经济经历了一段史上最繁荣的无通胀增长时期。达利欧分析了原因： 资金从借债国回流到美国，使美元升值，给美国经济造成了通缩压力，从而可以使美联储可以在不加剧通胀的情况下降息。 美联储借给了银行现金，同时国际金融组织重新做出了安排，使债务国能够以举借新债的方式履行还款义务，这相当于所有人都认为一切都是良好的，然后在接下来的许多年里逐渐的减少不良债务。 坦诚相待 塑造者 认识现实 5步流程实现人生愿望 人与人大不相同","tags":[]},{"title":"database-design","date":"2019-09-14T02:03:49.000Z","path":"2019/09/14/database-design/","text":"规范化","tags":[]},{"title":"记录一些ECMAScript6的语法","date":"2019-08-23T09:45:58.000Z","path":"2019/08/23/learn-ECMAScript6/","text":"ECMAScript6简称ES6。 变量的解构赋值ES6允许按照一定的模式从数组和对象中提取值，然后对变量进行赋值，这称为解构。 数组的解构赋值123456789101112//ES6之前let a = 1;let b = 2;let c = 3;console.log(a); //1//ES6之后let [a, b, c] = [1, 2, 3];console.log(a); //1let [ , , third] = [1, 2, 3];console.log(third); //3 对象的解构赋值123456789101112131415161718192021222324252627282930313233343536373839//代码-1let &#123;foo, bar&#125; = &#123;foo: \"aaa\", bar: \"bbb\"&#125;;console.log(foo); //aaaconsole.log(bar);//bbb//变量没有对应的同名属性，导致取不到值。let &#123;baz&#125; = &#123;foo: \"aaa\", bar: \"bbb\"&#125;;console.log(baz); //undefined//如果变量名和属性名不一致，必须使用如下语法let &#123;foo：baz&#125; = &#123;foo: \"aaa\", bar: \"bbb\"&#125;;console.log(baz); //aaa//代码-1是下面语法的简写，大括号中的前者是匹配的模式，后者是真正的变量。let &#123;foo: foo, bar: bar&#125; = &#123;foo: \"aaa\", bar: \"bbb\"&#125;;console.log(foo); //aaaconsole.log(bar);//bbb//解构嵌套解构的对象let obj = &#123; p: [ 'Hello', &#123;y: 'World'&#125; ]&#125;;//p是模式，后面的数组是变量；//x是变量，因为'Hello'是字面量，所以可以直接赋值给x；//y是与对象属性同名的变量，如果用其他变量名，必须在变量名前面声明模式。let &#123;p: [x, &#123;y&#125;]&#125; = obj;console.log(x);//Helloconsole.log(y);//World//多次解构赋值对象。//前面的p是第一次解构与对象属性名相同的变量。//后面的p是第二次解构的模式。let &#123;p, p: [x, &#123;y&#125;]&#125; = obj;console.log(p);//['Hello', &#123;y: 'World'&#125;]console.log(x);//Helloconsole.log(y);//World 字符串的解构赋值12345const [a, b, c, d, e] = 'hello';console.log(a);//hconsole.log(b);//econsole.log(c);//l... 数值和字符值的解构赋值12let &#123;toString: s&#125; = 1123;console.log(s)//ƒ toString() &#123; [native code] &#125; 解构赋值的用途交换变量123456let x = 1;let y = 2;[x, y] = [y, x];console.log(x);//2console.log(y);//1 函数返回多个值123456789101112131415161718192021//返回一个数组function example() &#123; return [1, 2, 3];&#125;let [a, b, c] = example();console.log(a);//1console.log(b);//2console.log(c);//3//返回一个对象function example() &#123; return &#123; foo: 1, bar: 2 &#125;;&#125;let &#123;foo, bar&#125; = example();console.log(foo);//1console.log(bar);//2 函数参数的定义123456789101112131415161718192021//传递数组function f([x, y, z]) &#123; console.log(x); console.log(y); console.log(z);&#125;f([1, 2, 3]]);//1//2//3//传递对象function f(&#123;x, y, z&#125;) &#123; console.log(x); console.log(y); console.log(z);&#125;f(&#123;'z': 3, 'y': 2, 'x': 1&#125;);//1//2//3 提取json数据12345678let jsonData = &#123; id: 42, status: \"OK\", data: [867,5039]&#125;;let &#123;id, status, data: number&#125; = jsonData;console.log(id, status, number);//42, \"OK\", [867, 5039] 函数参数的默认值12345678jQuery.ajax = function(url, &#123; async = true, beforeSend = function() &#123;&#125;, cache = true, //more config&#125;) &#123; //do stuff&#125;; 遍历map解构12345678var map = new Map();map.set('first', 'hello');map.set('second', 'wolrd');for (let [key, value] of map) &#123; console.log(key + \" is \" + value);&#125;//first is hello//second is world 输入模块的指定方法1const &#123;SourceMapConsumer, SourceNode&#125; = require(\"source-map\"); 字符串的扩展模板字符串1234567891011121314151617181920212223242526272829//传统的js写法：$('#result').append( 'There are &lt;b&gt;' + basket.count + '&lt;/b&gt; ' + 'items in your basket, ' + '&lt;em&gt;' + basket.onSale + '&lt;/em&gt; are on sale!');//es6的写法$('#result').append(` There are &lt;b&gt;$&#123;basket.count&#125;&lt;/b&gt; items in your basket, &lt;em&gt;$&#123;basket.onSale&#125;&lt;/em&gt; are on sale!`);var name = \"Bob\", time = \"today\";`hello $&#123;name&#125;, how are you $&#123;time&#125;?`//hello Bob, how are you today?/**模板中还能放表达式用于运算，以及引用对象属性。模板字符串中还能调用函数。*/var x = 1;var y = 2;`$&#123;x&#125; + $&#123;y&#125; = $&#123;x + y&#125;`//\"1 + 2 = 3\" Module的语法Module可以将大的程序拆分成互相依赖的小文件，再用简单的方法将它们拼接起来。ES6的模块默认使用严格模式，不管有没有在头部加”use strict”。 export命令模块功能只要有两个命令构成：export和import；export命令用于声明模块的对外接口，import命令用于输入其他模块提供的功能。 一个模块就是一个独立文件，文件内的变量，外部无法获取。如果外部想要读取模块内部的某个变量，模块必须使用export关键字输出该变量。 1234567891011121314151617181920212223//写法1//profile.jsexport var firstName = 'Michael';export var lastName = 'Jackson';export var year = 1983;//写法2//profile.jsvar firstName = 'Michael';var lastName = 'Jackson';var year = 1983;export &#123;firstName, lastName, year&#125;;//写法3//profile.jsvar v1 = 'Michael';var v2 = 'Jackson';var v3 = 1983;export &#123;v1 as firstName, v2 as lastName, v3 as year&#125;;//export还可以导出函数和类。 import命令使用了export定义了模块的对外接口后，在其它文件中就可以使用import加载这个模块了。123456789101112131415//写法1//main.jsimport &#123;firstName, lastName, year&#125; from './profile';function setName(element) &#123; element.textContent = firstName + ' ' + lastName;&#125;//写法2//为导入的变量声明别名import &#123;firstName as surnamae&#125; from './profile';/**import是静态执行，所以不能使用变量（let x）和表达式（x + 'a'），只有在运行时才能得到结果。*/ export default命令使用import命令时必须知道要加载的变量名或函数名，否则无法加载。为了能不使用变量名就能加载变量和函数，可以使用export default为模块指定默认输出。一个模块只能有一个export default。本质上export default就是输出一个叫做default的变量或方法，系统允许我们为它取任意的名字。 123456789//export-default.jsexport default function() &#123; console.log('foo');&#125;//import-default.js//加载该模块时，import命令可以为该匿名函数指定任意的名字，此时import后面不适用大括号。import customName from './export-default';customName();//foo export和import的复合写法1234export &#123;foo, bar&#125; from 'my_module';//等价于import &#123;foo, bar&#125; from 'my_module';export &#123;foo, bar&#125;;","tags":[]},{"title":"解析spring事务管理","date":"2019-08-08T10:49:58.000Z","path":"2019/08/08/spring-transaction-analyze/","text":"从使用和源码两方面来分析。","tags":[]},{"title":"分析spring ioc的实现","date":"2019-07-29T03:33:36.000Z","path":"2019/07/29/spring-ioc-analyze/","text":"1：bean的生命周期 2：IOC容器是如何解决循环依赖的 3：spring的哪中循环依赖是不能不解决的？ 为啥不能解决？ 1）org.springframework.context.support.AbstractApplicationContext#getbean 2）org.springframework.beans.factory.support.AbstractBeanfactorydogetbean 2.1）transformedbeanname解析我们的别名 2.2）getsingleton首先尝试去我们的缓存池中去获取对象，（由于是第一次创建Bean所以缓存池中没有对象） 2.3）若出现循环依赖，判断bean是不是单例的不是就抛出异常 2.4）当前容器是否有父工厂有由父工厂加教 2.5）合并我们的Bean定义 2.6）检查我们当前的bean定义是不是抽象的？ 2.7）做dependson的依赖检查 2.8）调用 getsingleton的方法 2.8.1）beforesingletoncreation标志当前的Bean正在被创建日 3）createbean创建Bean的流程 resolvebeforeinstantiation aop过程中找切面（ aspectj） 3.1）docreatebean：调用我们真正的创建bean的流程 4）org. springframework beans. factory support. Abstractautowirecapablebeanfactory#docreatebean 4.1）createbeaninstance（）调用我们的构造器来进行创建对象（属性还没有被赋值，）（早期对象） 4.2）提前暴露我们的早期对象..加入到我们的缓存中 创建A----去给我们的A中的属性B赋值的过程，发现依赖B对象然后 getbean（ Instb） 创建B的流程去给我们B中的属性赋值的时候，发现依赖A对象 》//解析bean》org.springframework.context.support.AbstractApplicationContext#invokeBeanFactoryPostProcessors 》》org.springframework.context.support.PostProcessorRegistrationDelegate#invokeBeanFactoryPostProcessors(org.springframework.beans.factory.config.ConfigurableListableBeanFactory, java.util.List&lt;org.springframework.beans.factory.config.BeanFactoryPostProcessor&gt;) 》》》org.springframework.context.support.PostProcessorRegistrationDelegate#invokeBeanDefinitionRegistryPostProcessors 》》》》org.springframework.context.annotation.ConfigurationClassPostProcessor#postProcessBeanDefinitionRegistry 》》》》》//找到Config类 》》》》》org.springframework.context.annotation.ConfigurationClassPostProcessor#processConfigBeanDefinitions 》》》》》》//解析Config类下的bean 》》》》》》org.springframework.context.annotation.ConfigurationClassParser#parse(java.util.Set&lt;org.springframework.beans.factory.config.BeanDefinitionHolder&gt;) 》》》》》》》//生成Config类所在的package 》》》》》》》org.springframework.context.annotation.ConfigurationClassParser#processConfigurationClass 》》》》》》》》//解析Config类所在的package 》》》》》》》》org.springframework.context.annotation.ComponentScanAnnotationParser#parse 》》》》》》》》》//扫描package下的class 》》》》》》》》》org.springframework.context.annotation.ClassPathBeanDefinitionScanner#doScan 》》》》》》》》》》//找到package下spring托管的bean，并初始化bean定义 》》》》》》》》》》org.springframework.context.annotation.ClassPathScanningCandidateComponentProvider#findCandidateComponents 》//初始化bean》org.springframework.context.support.AbstractApplicationContext#finishBeanFactoryInitialization 》》spring 容器初始化时调用“预实例化单例”方法 》》org.springframework.beans.factory.support.DefaultListableBeanFactory#preInstantiateSingletons 》》》根据bean名称获取容器中的bean 》》》org.springframework.beans.factory.support.AbstractBeanFactory#getBean(java.lang.String) 》》》》org.springframework.beans.factory.support.AbstractBeanFactory#doGetBean","tags":[]},{"title":"分析spring aop的实现","date":"2019-07-27T11:26:27.000Z","path":"2019/07/27/spring-aop-analyze/","text":"spring在启动的时候开始解析aop设置 》//开启Aspect后，注册AnnotationAwareAspectJAutoProxyCreator，他根据@Pointcut注解定义的切点来自动代理相匹配的bean。》org.springframework.aop.config.AopConfigUtils#registerAspectJAnnotationAutoProxyCreatorIfNecessary(org.springframework.beans.factory.support.BeanDefinitionRegistry, java.lang.Object) 》》//spring加载bean时，为bean创建代理的入口 》》org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator#postProcessAfterInitialization 》》》//判断是否需要增强 》》》org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator#wrapIfNecessary 》》》》//1，获取增强器 》》》》org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator#getAdvicesAndAdvisorsForBean 》》》》》//遍历所有的bean，找到所有的切面（@Aspect） 》》》》》org.springframework.aop.aspectj.annotation.BeanFactoryAspectJAdvisorsBuilder#buildAspectJAdvisors 》》》》》》//获取切面中的通知（@Before、@After等）信息 》》》》》》org.springframework.aop.aspectj.annotation.ReflectiveAspectJAdvisorFactory#getAdvisor 》》》》》》》//根据通知信息生成增强 》》》》》》》org.springframework.aop.aspectj.annotation.InstantiationModelAwarePointcutAdvisorImpl#InstantiationModelAwarePointcutAdvisorImpl 》》》》》》》》//根据不同通知类型生成增强（Advice） 》》》》》》》》//生成的增强方法会由对应的Interceptor执行 》》》》》》》》//例如AspectJMethodBeforeAdvice中的方法会由org.springframework.aop.framework.adapter.MethodBeforeAdviceInterceptor#invoke执行 》》》》》》》》//Interceptor会在？ 》》》》》》》》org.springframework.aop.aspectj.annotation.ReflectiveAspectJAdvisorFactory#getAdvice 》》》》》//2，匹配增强器，寻找增强器中适用于当前class的增强器 》》》》》org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator#findAdvisorsThatCanApply 》》》》》》//引介增强和普通的增强处理不一样，所以会分开处理 》》》》》》org.springframework.aop.support.AopUtils#canApply(org.springframework.aop.Advisor, java.lang.Class&lt;?》, boolean) 》》》》//3，创建代理 》》》》org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator#createProxy 》//配置两个参数》//1，是否强制使用cglib：proxy-target-class》//2，是否暴露代理对象：expose-proxy；用于被jdk代理的目标对象的自调用方法。》org.springframework.context.annotation.AspectJAutoProxyRegistrar#registerBeanDefinitions","tags":[]},{"title":"【转】一些数学问题","date":"2019-07-13T13:14:42.000Z","path":"2019/07/13/mathematical-application/","text":"为什么n个二进制可表示的最小数为0，最大数为2的n次方减1？ 4位10进制里，可表示的最小数是0，最大数是10的4次方减1。类比推理，n个k进制里，可表示的最小数是0，最大数是k的n次方减1。道理很简单，k进制里，每一位有0到k-1共计k种状态，共n位，按全排列算法，共有k的n次方种状态，因为包含全零状态，所以最大的就是k的n次方减1了。 如何快速判断正整数是2的N次幂？ https://www.iteblog.com/archives/716.html 这个问题可能很多面试的人都遇到过，很多人可能想利用循环来判断，代码可能如下所示： 12345678910public static boolean isPowOfTwo(int n) &#123; int temp = 0; for (int i = 1; ; i++) &#123; temp = (int) Math.pow(2, i); if (temp &gt;= n) break; &#125; if (temp == n) return true; else return false;&#125; 上面的代码简单明了。但是，这样的方案效率比较低。我们仔细分析一下，正整数是2的n次幂他有什么规律？20=1,21=2,22=4,23=8….这样看是没有什么规律的。但是如果将2的幂次方写成二进制形式后，很容易就会发现有以下两个特点： 1、20=1 -&gt; 0001,21=2 -&gt; 0010,22=4 -&gt; 0100,23=8 -&gt; 1000二进制中只有一个1，并且1后面跟了n个0。 2、如果将这个数减去1后会发现，仅有的那个1会变为0，而原来的那n个0会变为1；因此将原来的数与去减去1后的数字进行与运算后会发现为零（(x &amp; x- 1) == 0）。 原因：因为2n换算是二进制为10……0这样的形式，2n-1的二进制为0111…1，两个二进制求与结果为0，例如：16的二进制为10000；15=01111，两者相与的结果为0。计算如下： 1234 10000&amp;01111 ------- 00000 所以可以用下面算法实现： 123public static boolean isPowerOfTwo(int x) &#123; return x &gt; 0 &amp; (x &amp; (x - 1)) == 0;&#125; 很简单的一行代码就实现了。细心的读者可能会问：2的n次幂二进制始终都是只有一个1，其它的位数都为0，是否可以判断给定的数转换为二进制来判断其中是否只有1个1来得出给定数是否为2的n次幂呢？答案是不能。因为Integer.MIN_VALUE的二进制只有1个1，但是Integer.MIN_VALUE并不是2的n次幂，所以不能用上面方式来实现。","tags":[]},{"title":"用jenkins配置一个Freestyle project","date":"2019-07-06T14:27:33.000Z","path":"2019/07/06/jenkins-settings/","text":"本例中Jenkins服务器和gitlab的地址都是192.168.152.131；应用服务器地址是192.168.152.128. 主要可以分为一下几个步骤： 1，配置代码仓库这一步用于拉取源代码 下面是配置gitlab的访问权限，其中的private key是Jenkins服务器的。 下面还要在gitlab中配置Jenkins服务器的public key。 2，配置Build这一步用于编译并构建代码，构建方式取决于构建工具。 3，配置Post-build Actions这一步用于运行构建后的应用；这里使用的是Publish Over SSH插件，该插件需要自己下载安装，它可以将构建后的文件发送到远程的应用服务器上，并且还能执行远程服务器上的脚本。当然这一步也可以选择其他的插件，比如：SSH plugin，但是它只能执行远程服务器脚本。 安装完插件后，首先需要在Configure System中配置以下信息；其中的ssh server可能会连接失败，那么需要在Jenkins服务器上执行一个命令：ssh-copy-id danny@192.168.152.128。 下面是配置构建后的操作","tags":[]},{"title":"【转】为什么Java中只有值传递","date":"2019-06-29T10:45:10.000Z","path":"2019/06/29/java-core-transfer/","text":"为什么Java中只有值传递 原文： https://github.com/Snailclimb/JavaGuide/blob/master/docs/essential-content-for-interview/MostCommonJavaInterviewQuestions/第一周（2018-8-7）.md 首先回顾一下在程序设计语言中有关将参数传递给方法（或函数）的一些专业术语。按值调用(call by value)表示方法接收的是调用者提供的值，而按引用调用（call by reference)表示方法接收的是调用者提供的变量地址。一个方法可以修改传递引用所对应的变量值，而不能修改传递值调用所对应的变量值。 它用来描述各种程序设计语言（不只是Java)中方法参数传递方式。 Java程序设计语言总是采用按值调用。也就是说，方法得到的是所有参数值的一个拷贝，也就是说，方法不能修改传递给它的任何参数变量的内容。 下面通过 3 个例子来给大家说明 example 1123456789101112131415161718public static void main(String[] args) &#123; int num1 = 10; int num2 = 20; swap(num1, num2); System.out.println(\"num1 = \" + num1); System.out.println(\"num2 = \" + num2);&#125;public static void swap(int a, int b) &#123; int temp = a; a = b; b = temp; System.out.println(\"a = \" + a); System.out.println(\"b = \" + b);&#125; 结果： 1234a = 20b = 10num1 = 10num2 = 20 解析： 在swap方法中，a、b的值进行交换，并不会影响到 num1、num2。因为，a、b中的值，只是从 num1、num2 的复制过来的。也就是说，a、b相当于num1、num2 的副本，副本的内容无论怎么修改，都不会影响到原件本身。 通过上面例子，我们已经知道了一个方法不能修改一个基本数据类型的参数，而对象引用作为参数就不一样，请看 example2. example 21234567891011public static void main(String[] args) &#123; int[] arr = &#123; 1, 2, 3, 4, 5 &#125;; System.out.println(arr[0]); change(arr); System.out.println(arr[0]);&#125;public static void change(int[] array) &#123; // 将数组的第一个元素变为0 array[0] = 0;&#125; 结果： 1210 解析： array 被初始化 arr 的拷贝也就是一个对象的引用，也就是说 array 和 arr 指向的时同一个数组对象。 因此，外部对引用对象的改变会反映到所对应的对象上。 通过 example2 我们已经看到，实现一个改变对象参数状态的方法并不是一件难事。理由很简单，方法得到的是对象引用的拷贝，对象引用及其他的拷贝同时引用同一个对象。 很多程序设计语言（特别是，C++和Pascal)提供了两种参数传递的方式：值调用和引用调用。有些程序员（甚至本书的作者）认为Java程序设计语言对对象采用的是引用调用，实际上，这种理解是不对的。由于这种误解具有一定的普遍性，所以下面给出一个反例来详细地阐述一下这个问题。 example 312345678910111213141516171819public class Test &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub Student s1 = new Student(\"小张\"); Student s2 = new Student(\"小李\"); Test.swap(s1, s2); System.out.println(\"s1:\" + s1.getName()); System.out.println(\"s2:\" + s2.getName()); &#125; public static void swap(Student x, Student y) &#123; Student temp = x; x = y; y = temp; System.out.println(\"x:\" + x.getName()); System.out.println(\"y:\" + y.getName()); &#125;&#125; 结果： 1234x:小李y:小张s1:小张s2:小李 解析： 交换之前： 交换之后： 通过上面两张图可以很清晰的看出： 方法并没有改变存储在变量 s1 和 s2 中的对象引用。swap方法的参数x和y被初始化为两个对象引用的拷贝，这个方法交换的是这两个拷贝 总结Java程序设计语言对对象采用的不是引用调用，实际上，对象引用是按值传递的。 下面再总结一下Java中方法参数的使用情况： 一个方法不能修改一个基本数据类型的参数（即数值型或布尔型）。 一个方法可以改变一个对象参数的状态。 一个方法不能让对象参数引用一个新的对象。","tags":[]},{"title":"java中的泛型","date":"2019-06-29T03:52:39.000Z","path":"2019/06/29/java-core-generic/","text":"泛型实现了参数化类型的概念，使代码可以应用于多种类型；使程序有更好的可读性和安全性。 可读性：可以很直观的看到泛型类或方法接收什么类型的参数安全性：在使用泛型是必须指定类型参数，避免了在获取类型参数时产生的类型转换异常。 泛型类1234567891011121314151617/** * 这是一个泛型类，T就是类型参数 * @param &lt;T&gt; */public class Tool&lt;T&gt; &#123; private T a; public T get() &#123; return this.a; &#125; public void set(T a) &#123; this.a = a; &#125;&#125; 泛型接口1234public interface Generator&lt;T&gt; &#123; //返回类型是参数化的T T next();&#125; 12345678910111213141516171819202122232425public class Fibonacci implements Generator&lt;Integer&gt; &#123; private int count = 0; /** * 参数化的Generator接口确保next的返回值是参数的类型。 * @return */ @Override public Integer next() &#123; return fib(count++); &#125; private int fib(int n) &#123; if (n &lt; 1) &#123; return 1; &#125; return fib(n - 2) + fib(n - 1); &#125; public static void main(String[] args) &#123; Fibonacci gen = new Fibonacci(); for (int i = 0; i &lt; 8; i++) &#123; System.out.println(gen.next() + \"\"); &#125; &#125;&#125; 泛型方法泛型方法使得方法能够独立于类而产生定义上的变化。 如果泛型方法可以取代整个类的泛型化，就应该尽量使用泛型方法。 static方法不能访问类的类型参数，所以static方法如果要使用泛型能力，必须将其定义为独立的泛型方法。 123456789101112public class GenericMethods &#123; public &lt;T&gt; void f(T x) &#123; System.out.println(x.getClass().getName()); &#125; public static void main(String[] args) &#123; GenericMethods gm = new GenericMethods(); gm.f(\"\"); gm.f(1); gm.f(gm); &#125;&#125; 擦除 在泛型代码内部，无法获取泛型参数类型的任何信息。 ArrayList.class 替换为new ArrayList().getClass()。","tags":[]},{"title":"java中HashMap源码分析","date":"2019-06-07T02:26:54.000Z","path":"2019/06/07/java-core-hashmap/","text":"HashMap源码解析 存放新的map1234567891011121314151617181920212223242526final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; //新map存放元素的个数 int s = m.size(); if (s &gt; 0) &#123; //原始map未初始化，也就是还没有元素 if (table == null) &#123; // pre-size //因为threshold = capacity * loadFactor，此时的threshold相当于就是元素的大小。 //所以下面的计算相当于ft = threshold / loadFactor，ft也就是capacity。 float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); //容量大于阈值，则初始化阈值 if (t &gt; threshold) threshold = tableSizeFor(t); &#125; //新map的元素个数大于阈值，需要扩容 else if (s &gt; threshold) resize(); //新map的添加到原始map中 for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125; &#125; put存放新的元素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;/** */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //原始table未初始化或长度为0，需要调用扩容方法 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; /** (n - 1) &amp; hash也即是hash值，用来定位key在数组中的位置，如果该位置为空，就创建新的node，并放在数组中。 (n - 1) &amp; hash保证获取的index一定在数组范围内，举个例子，默认容量16，n-1=15，hash=18,转换成二进制计算为 1 0 0 1 0 &amp; 0 1 1 1 1 __________________ 0 0 0 1 0 = 2 */ if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); //数组里面有元素了 else &#123; Node&lt;K,V&gt; e; K k; //用查到的元素，也即是数组中的第一个元素；与加入的key做比较，比较他们的hash值、地址值。 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) //如果相等就用一个临时变量记录查到的元素。 e = p; //如果key不相等，并且查到的元素是个红黑树的节点，就将key和value放到红黑树中。 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; //如果key不相等，并且查到元素的不是红黑树的节点，那就是链表的节点。 //下面就是遍历节点所在的链表。 for (int binCount = 0; ; ++binCount) &#123; //遍历到链表的尾部 if ((e = p.next) == null) &#123; //将加入的key和value组成节点放到链表尾部 p.next = newNode(hash, key, value, null); //节点数量达到红黑树阈值，将链表转成红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //如果链表中存在与新加入的key相等的节点，则跳出遍历，丢弃新加入的key if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //如果数组中有和新加入的key相等的节点 if (e != null) &#123; // existing mapping for key V oldValue = e.value; //用新的value替换旧的value if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; //如果当前元素数量大于阈值，则进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; resize扩容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //数组中有元素时，进行扩容。 if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //创建HashMap时，指定了capacity和loadFactor，即调用了new HashMap(int initialCapacity)时，将threshold赋给capacity。 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; //创建默认的HashMap，即调用了new HashMap()时，设置默认的capacity和threshold。 else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //创建HashMap时，指定了capacity和loadFactor，即调用了new HashMap(int initialCapacity)时，设置新的threshold。 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; //用新的数组capacity初始化新的数组 @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; //开始遍历原数组，进行数据迁移。 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; //元素是单节点，直接复制元素到新的数组。 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //元素是红黑树节点 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //元素是链表 else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; /** 进行链表复制。 将原链表拆分为两个链表，使元素在新的数组中分布的更松散。 如果 (e.hash &amp; oldCap) == 0 则该节点在新表的下标位置与旧表一致都为 j 如果 (e.hash &amp; oldCap) == 1 则该节点在新表的下标位置 j + oldCap */ if ((e.hash &amp; oldCap) == 0) &#123; //保持元素在原链表中的顺序。 if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 为何HashMap的数组长度一定是2的次幂？ 数组长度保持2的次幂，length-1的低位都为1，会使得获得的数组索引index更加均匀。 https://www.cnblogs.com/chengxiao/p/6059914.htmlhttps://segmentfault.com/a/1190000015812438https://blog.csdn.net/login_sonata/article/details/76598675","tags":[]},{"title":"sql中的联结查询","date":"2019-05-09T14:08:55.000Z","path":"2019/05/09/sql-join-query/","text":"联结查询是关系型数据库中对多个关系表进行操作的重要功能。联结的方式可以分为以下几种。 涉及到的表结构： user表 orders表 order_items表 默认的联结查询 第一个表中的每一行将与第二个表中的每一行配对。这时返回的结果集被称为笛卡尔积；即检索出的行数等于第一个表中的行数乘以第二个表中的行数；也称为叉联结（cross join）。 等值联结 通常情况下，联结查询都需要联结条件-where语句；这样才能过滤出正确的两个关系表中的数据。 内联结 它是等值联结的标准语法；等同于等值联结。 如果从表中有主表的重复的关联值，则查询主表时，会重复出现主表中的数据例如查询uses时，由于orders中有重复的users的ID，所以查询时会有重复的users的数据 自联结 用于同一个表的联结查询，需要用到表的别名。上面语句的意思是找到订单额是2的用户的所有订单；SQL的执行顺序是from语句组成笛卡尔积，where语句从左到右过滤出需要的数据。 自然联结外联结用于查询在相关表中没有关联行的行。 全外联结MySQL不支持","tags":[]},{"title":"mybatis架构中的一些概念","date":"2019-01-02T13:05:27.000Z","path":"2019/01/02/mybatis-principle/","text":"从三个框架中的具体处理来理解：mybatis，mybatis-spring，mybatis-spring-boot-autoconfigure mybatismybatis中基础对象的scope和lifecycle理解不同作用域和生命周期类是至关重要的，因为错误的使用会导致并发问题。 1，SqlSessionFactoryBuilder这个类可以被实例化、使用和丢弃，一旦创建了SqlSessionFactory，就不再需要它了。因此SqlSessionFactoryBuilder实例的最佳作用域是method作用域。 2，SqlSessionFactorySqlSessionFactory一旦被创建就应该在application的运行期间一直存在，没有任何理由对它进行清除或重建。使用SqlSessionFactory的最佳实践是在application运行期间不要重复创建多次：SqlSessionFactory的最佳作用域是application作用域 3，SqlSession每个线程都应该有它自己的SqlSession实例。SqlSession的实例不是线程安全的，因此是不能被共享的，所以它的最佳的作用域是request或者method作用域。 SqlSession相当于一个JDBC的Connection对象，也可以说它的生命周期是在请求数据库处理事务的过程中，它的长期存在对数据库连接池影响很大，应该及时close。它存活于一个应用的请求和操作，可以执行多条SQL，保证事务的一致性。 4，Mapper Instances映射器是映射的其实就是SQL语句；供SqlSession调用。映射器接口的实例是用SqlSession中从Mybatis的Configuration对象中获得的。因此，任何映射器实例的最大作用域是和请求它们的 SqlSession 相同的，并且映射器实例的最佳作用域是method作用域。也就是说，映射器实例应该在调用它们的方法中被请求，用过之后即可废弃，并不需要显式地关闭映射器实例。 事务Mybatis使用事务的前提配置：12345678910111213&lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;!--定义的环境 ID--&gt; &lt;transactionManager type=\"JDBC\"&gt; &lt;!--事务管理器--&gt; &lt;property name=\"...\" value=\"...\"/&gt; &lt;/transactionManager&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;!--数据源的配置--&gt; &lt;property name=\"driver\" value=\"$&#123;driver&#125;\"/&gt; &lt;property name=\"url\" value=\"$&#123;url&#125;\"/&gt; &lt;property name=\"username\" value=\"$&#123;username&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;password&#125;\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt;&lt;/environments&gt; 参考：http://www.mybatis.org/mybatis-3/zh/configuration.html#environments 从入门代码分析整个事务的处理逻辑： 1，SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); 12345678910111213141516171819/**@see org.apache.ibatis.session.SqlSessionFactoryBuilder**/public SqlSessionFactory build(InputStream inputStream, String environment, Properties properties) &#123; try &#123; XMLConfigBuilder parser = new XMLConfigBuilder(inputStream, environment, properties); //调用build方法。 return build(parser.parse()); &#125; catch (Exception e) &#123; ... &#125; finally &#123; ... &#125;&#125; ...public SqlSessionFactory build(Configuration config) &#123; //创建默认的SqlSessionFactory。 return new DefaultSqlSessionFactory(config);&#125; 2，SqlSession session = sqlSessionFactory.openSession(); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/**@see org.apache.ibatis.session.defaults.DefaultSqlSessionFactory**/public SqlSession openSession() &#123; return openSessionFromDataSource(configuration.getDefaultExecutorType(), null, false);&#125;...private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; final Environment environment = configuration.getEnvironment(); //根据配置获取事务管理器 final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); //创建Transaction并设置DataSource、隔离级别、是否自动提交。 tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); //创建Executor，它里面有commit、query等方法。 final Executor executor = configuration.newExecutor(tx, execType); //创建默认的SqlSession。并设置Executor等属性。 return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123; ... &#125; ...&#125;private TransactionFactory getTransactionFactoryFromEnvironment(Environment environment) &#123; /** 根据transactionManager节点的type获取TransactionFactory。 @see org.apache.ibatis.session.Configuration#Configuration()。 Mybatis有两种管理事务的方式： 1，JdbcTransaction；它使用JDBC的事务提交和回滚机制。直观地讲，就是JdbcTransaction使用的是java.sql.Connection上的commit和rollback功能， JdbcTransaction只是相当于对java.sql.Connection事务处理进行了一次包装（wrapper）。 2，ManagedTransaction，它是将事务的提交和回滚操作交给具体的容器去实现的；直接调用ManagedTransaction中的commit和rollback方法是无效的。 具体的实现有：mybatis-spring中的SpringManagedTransaction等。 **/ if (environment == null || environment.getTransactionFactory() == null) &#123; return new ManagedTransactionFactory(); &#125; return environment.getTransactionFactory();&#125;/**@see org.apache.ibatis.session.Configuration#newExecutor()**/public Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123; ... Executor executor; if (ExecutorType.BATCH == executorType) &#123; ... &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125; if (cacheEnabled) &#123; executor = new CachingExecutor(executor); &#125; ... return executor;&#125; 3，Blog blog = (Blog) session.selectOne(&quot;org.mybatis.example.BlogMapper.selectBlog&quot;, 101); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/**SqlSession的SQL的具体执行过程是委托给Executor的。**/public &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) &#123; try &#123; MappedStatement ms = configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); &#125; catch (Exception e) &#123; ... &#125;&#125;/**默认开启缓存的话是CachingExecutor负责执行SQL的。@see org.apache.ibatis.executor.CachingExecutor#query()**/public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; Cache cache = ms.getCache(); if (cache != null) &#123; flushCacheIfRequired(ms); if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123; ... List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key); if (list == null) &#123; list = delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); tcm.putObject(cache, key, list); // issue #578 and #116 &#125; return list; &#125; &#125; return delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);&#125;/**查询数据库会委托给BaseExecutor和SimpleExecutor@see org.apache.ibatis.executor.BaseExecutor#query()@see org.apache.ibatis.executor.BaseExecutor#queryFromDatabase()**/public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; ... List&lt;E&gt; list; try &#123; queryStack++; list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null; if (list != null) &#123; handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); &#125; else &#123; list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql); &#125; &#125; finally &#123; queryStack--; &#125; ... &#125; return list;&#125;/**SimpleExecutor会调用JDBC的API。@see org.apache.ibatis.executor.SimpleExecutor#doQuery()**/public &lt;E&gt; List&lt;E&gt; doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException &#123; Statement stmt = null; try &#123; Configuration configuration = ms.getConfiguration(); StatementHandler handler = configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql); stmt = prepareStatement(handler, ms.getStatementLog()); return handler.query(stmt, resultHandler); &#125; finally &#123; closeStatement(stmt); &#125;&#125; 或者 4，BlogMapper mapper = session.getMapper(BlogMapper.class); Blog blog = mapper.selectBlog(101); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/**使用Mapper的方式执行SQL会用到动态代理；最后还是会回到SqlSession中去执行SQL。@see org.apache.ibatis.session.defaults.DefaultSqlSession#getMapper()**/public &lt;T&gt; T getMapper(Class&lt;T&gt; type) &#123; return configuration.&lt;T&gt;getMapper(type, this);&#125;/**@see org.apache.ibatis.session.Configuration#getMapper()**/public &lt;T&gt; T getMapper(Class&lt;T&gt; type, SqlSession sqlSession) &#123; return mapperRegistry.getMapper(type, sqlSession);&#125;/**@see org.apache.ibatis.binding.MapperRegistry#getMapper()**/public &lt;T&gt; T getMapper(Class&lt;T&gt; type, SqlSession sqlSession) &#123; final MapperProxyFactory&lt;T&gt; mapperProxyFactory = (MapperProxyFactory&lt;T&gt;) knownMappers.get(type); ... try &#123; return mapperProxyFactory.newInstance(sqlSession); &#125; catch (Exception e) &#123; ... &#125;&#125;/**@see org.apache.ibatis.binding.MapperProxyFactory**/protected T newInstance(MapperProxy&lt;T&gt; mapperProxy) &#123; return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] &#123; mapperInterface &#125;, mapperProxy);&#125;public T newInstance(SqlSession sqlSession) &#123; final MapperProxy&lt;T&gt; mapperProxy = new MapperProxy&lt;&gt;(sqlSession, mapperInterface, methodCache); return newInstance(mapperProxy);&#125;/**Mapper代理类的详细执行过程可以看以下代码：@see org.apache.ibatis.binding.MapperProxy@see org.apache.ibatis.binding.MapperMethod#execute()最终还是回到SqlSession.selectXxx的逻辑中。**/ mybatis-springSqlSessionFactoryBean在基本的MyBatis中，SqlSessionFactory可以使用SqlSessionFactoryBuilder来创建而在myBatis-spring中,则使用SqlSessionFactoryBean来替代。 SqlSessionTemplateSqlSessionTemplate是myBatis-spring的核心。这个类负责管理MyBatis的SqlSession，调用MyBatis的SQL方法。SqlSessionTemplate是线程安全的，可以被多个DAO所共享使用；SqlSessionTemplate将会保证使用的SqlSession是和当前Spring的事务相关的。 Mapper为了代替手工使用SqlSessionDaoSupport或SqlSessionTemplate编写数据访问对象(DAO)的代码，myBatis-spring提供了一个动态代理的实现：MapperFactoryBean。这个类可以直接注入Mapper接口到service层bean中。当使用Mapper时，直接调用就可以了，不需要编写任何实现的代码，因为myBatis-spring将会为你创建代理。 MapperFactoryBean创建的代理控制打开和关闭session，还能转换任意的异常到Spring的DataAccessException异常中。此外,如果要加入到一个已经存在活动事务中，代理将会开启一个新的 Spring 事务。 事务mybatis-spring使用事务的前提配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;!--将DataSource注入到SqlSessionFactoryBean中--&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;!--指定mybatis的Mapper对应的XML文件的位置。--&gt; &lt;!--还有一种配置方式是在mybatis的XML配置文件中使用&lt;mappers&gt;部分来指定类路径。--&gt; &lt;property name=\"mapperLocations\" value=\"classpath*:sample/config/mappers/**/*.xml\" /&gt;&lt;/bean&gt;&lt;!--开启Spring的事务处理，配置之后，就可以在Spring中用@Transactional注解和AOP样式的配置来管理事务--&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt;&lt;/bean&gt;&lt;!--&lt;!--第一种方式：配置SqlSessionTemplate，代替mybatis默认实现的DefaultSqlSession--&gt;&lt;bean id=\"sqlSession\" class=\"org.mybatis.spring.SqlSessionTemplate\"&gt; &lt;constructor-arg index=\"0\" ref=\"sqlSessionFactory\" /&gt;&lt;/bean&gt;&lt;!--在DAO中注入SqlSessionTemplate，也就是SqlSession；这样DAO中就可以直接使用SqlSession了。--&gt;&lt;bean id=\"userDao\" class=\"org.xxx.xxx.dao.UserDaoImpl\"&gt; &lt;property name=\"sqlSession\" ref=\"sqlSession\" /&gt;&lt;/bean&gt;--&gt;&lt;!--&lt;!--第二种方式：如果不想用聚合SqlSessionTemplate或继承SqlSessionDaoSupport方式显式调用SqlSession，可以使用MapperFactoryBean生成Mapper接口的代理的方式调用数据访问api。--&gt;&lt;bean id=\"userMapper\" class=\"org.mybatis.spring.mapper.MapperFactoryBean\"&gt; &lt;!--MapperFactoryBean会为UserMapper创建代理。--&gt; &lt;property name=\"mapperInterface\" value=\"org.xxx.xxx.mapper.UserMapper\" /&gt; &lt;property name=\"sqlSessionFactory\" ref=\"sqlSessionFactory\" /&gt;&lt;/bean&gt;&lt;bean id=\"fooService\" class=\"org.xxx.xxx.mapper.FooServiceImpl\"&gt; &lt;property name=\"userMapper\" ref=\"userMapper\" /&gt;&lt;/bean&gt;--&gt;&lt;!--&lt;!--第三种方式：如果不想以上面XML的方式配置每个Mapper，可以使用MapperScannerConfigurer；它将会查找包路径下的Mapper并自动将它们创建成MapperFactoryBean--&gt;&lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;property name=\"basePackage\" value=\"org.xxx.xx.mapper\" /&gt;&lt;/bean&gt;--&gt; 接下来看看以上各个类的部分源码： SqlSessionFactoryBean创建SqlSessionFactory的过程： 12345678910111213141516171819202122232425262728293031323334/**@see org.mybatis.spring.SqlSessionFactoryBean#buildSqlSessionFactory()**/protected SqlSessionFactory buildSqlSessionFactory() throws IOException &#123; Configuration configuration; XMLConfigBuilder xmlConfigBuilder = null; //主要是读取配置文件的内容 ... //默认用SpringManagedTransactionFactory代替mybatis的TransactionFactory if (this.transactionFactory == null) &#123; this.transactionFactory = new SpringManagedTransactionFactory(); &#125; ... configuration.setEnvironment(new Environment(this.environment, this.transactionFactory, this.dataSource)); ... if (!isEmpty(this.mapperLocations)) &#123; ... for (Resource mapperLocation : this.mapperLocations) &#123; XMLMapperBuilder xmlMapperBuilder = new XMLMapperBuilder(mapperLocation.getInputStream(), configuration, mapperLocation.toString(), configuration.getSqlFragments()); xmlMapperBuilder.parse(); &#125; ... &#125; //使用mybatis的SqlSessionFactoryBuilder创建SqlSessionFactory。 return this.sqlSessionFactoryBuilder.build(configuration);&#125; SqlSessionTemplate代替SqlSession的过程： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/**SqlSessionTemplate在实例化Bean时会创建SqlSession的Proxy；调用Template的方法也即是在调用Proxy的方法。@see org.mybatis.spring.SqlSessionTemplate**/public SqlSessionTemplate(SqlSessionFactory sqlSessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator) &#123; ... this.sqlSessionFactory = sqlSessionFactory; this.executorType = executorType; this.exceptionTranslator = exceptionTranslator; //使用JDK的动态代理，创建SqlSession的Proxy。 //SqlSessionTemplate中各种访问数据库的方法都是由SqlSessionProxy负责执行的。 this.sqlSessionProxy = (SqlSession) newProxyInstance( SqlSessionFactory.class.getClassLoader(), new Class[] &#123; SqlSession.class &#125;, new SqlSessionInterceptor()); &#125; /**这是SqlSession的代理实现类@see org.mybatis.spring.SqlSessionTemplate.SqlSessionInterceptor**/private class SqlSessionInterceptor implements InvocationHandler &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; /** 静态导入的@see org.mybatis.spring.SqlSessionUtils.getSqlSession方法是spring管理mybatis事务的关键之处。 它的作用是:首先从当前事务的线程中获取SqlSession；如果有，直接返回SqlSession； 如果没有，则用SqlSessionFactory创建一个，并绑定到当前事务的线程中，即spring的事务管理器中。 **/ SqlSession sqlSession = getSqlSession( SqlSessionTemplate.this.sqlSessionFactory, SqlSessionTemplate.this.executorType, SqlSessionTemplate.this.exceptionTranslator); try &#123; Object result = method.invoke(sqlSession, args); ... return result; &#125; catch (Throwable t) &#123; ... &#125; finally &#123; if (sqlSession != null) &#123; closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); &#125; &#125; &#125;&#125; MapperFactoryBean生成Mapper代理的过程： 123456789101112131415161718192021222324252627/**该类通过继承SqlSessionDaoSupport，间接实现了InitializingBean，它的afterPropertiesSet方法会调用checkDaoConfig。@see org.mybatis.spring.mapper.MapperFactoryBean#checkDaoConfig**/protected void checkDaoConfig() &#123; super.checkDaoConfig(); ... Configuration configuration = getSqlSession().getConfiguration(); if (this.addToConfig &amp;&amp; !configuration.hasMapper(this.mapperInterface)) &#123; try &#123; /** 注册Mapper接口，其中会解析Mapper接口对应的SQL语句，在调用SqlSession的方法时用。 **/ configuration.addMapper(this.mapperInterface); &#125; catch (Exception e) &#123; ... &#125; &#125;&#125;/**获取Mapper对象时，最终还是回到Mybatis的SqlSession#getMapper()逻辑中。**/@Overridepublic T getObject() throws Exception &#123; return getSqlSession().getMapper(this.mapperInterface);&#125; MapperScannerConfigurer扫描并注册Mapper的过程： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/**@see org.mybatis.spring.mapper.MapperScannerConfigurer#postProcessBeanDefinitionRegistry()**/public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) &#123; if (this.processPropertyPlaceHolders) &#123; processPropertyPlaceHolders(); &#125; ClassPathMapperScanner scanner = new ClassPathMapperScanner(registry); //设置配置文件中的各种信息。 scanner.setAddToConfig(this.addToConfig); ... scanner.registerFilters(); scanner.scan(StringUtils.tokenizeToStringArray(this.basePackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS));&#125;/**@see org.mybatis.spring.mapper.ClassPathMapperScanner#doScan()**/public Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123; Set&lt;BeanDefinitionHolder&gt; beanDefinitions = super.doScan(basePackages); if (beanDefinitions.isEmpty()) &#123; LOGGER.warn(() -&gt; \"No MyBatis mapper was found in '\" + Arrays.toString(basePackages) + \"' package. Please check your configuration.\"); &#125; else &#123; processBeanDefinitions(beanDefinitions); &#125; return beanDefinitions;&#125;private void processBeanDefinitions(Set&lt;BeanDefinitionHolder&gt; beanDefinitions) &#123; GenericBeanDefinition definition; for (BeanDefinitionHolder holder : beanDefinitions) &#123; definition = (GenericBeanDefinition) holder.getBeanDefinition(); ... //将Mapper转换成MapperFactoryBean，MapperFactoryBean会负责生成Mapper的代理。 definition.setBeanClass(this.mapperFactoryBean.getClass()); ... //将SqlSessionFactory、SqlSessionTemplate等设置到BeanDefinition，也即是MapperFactoryBean中。 definition.getPropertyValues().add(\"sqlSessionFactory\", this.sqlSessionFactory); ... definition.getPropertyValues().add(\"sqlSessionTemplate\", this.sqlSessionTemplate); &#125;&#125; mybatis-spring-boot-autoconfigure它的主要功能是： 自动装配SqlSessionFactory 自动装配SqlSessionTemplate 用AutoConfiguredMapperScannerRegistrar代替MapperScannerConfigurer的功能。","tags":[]},{"title":"solr的使用","date":"2018-12-04T16:34:39.000Z","path":"2018/12/05/solr-usage/","text":"1，配置手动创建corecore的目录结构和相关配置文件的位置是特定的。solr启动时会扫描solr_home文件的带有core.properties文件的子目录。具体形式可以参考以下结构： 12345678910111213141516171819202122232425262728E:\\PROGRAM FILES\\SOLR-7.5.0 //solr的install目录└─server ├─solr │ │ README.md //从此文件中可知：该目录是默认的solr_home。solr_home也可以自定义为其他的目录 │ │ solr.xml │ │ zoo.cfg │ │ │ └─news-core //自定义的core。以下是core的配置文件 │ │ core.properties //定义core的基本信息。 │ │ │ ├─conf │ │ │ managed-schema //定义core相关Field，只能用Schema API更改其内容；与sheme.xml文件功能相同。 │ │ │ params.json │ │ │ protwords.txt │ │ │ solrconfig.xml │ │ │ stopwords.txt │ │ │ synonyms.txt │ │ │ │ │ └─lang │ │ contractions_ca.txt │ │ ...... │ │ userdict_ja.txt │ │ │ └─data │ ├─index │ │ ...... │ ├─...... 2，索引方式1：使用Request Handler123456789curl http://localhost:8983/solr/news-core/update -H &quot;Content-Type: text/xml&quot; --data-binary &apos;&lt;add&gt; &lt;doc&gt; &lt;field name=&quot;id&quot;&gt;1&lt;/field&gt; &lt;field name=&quot;title&quot;&gt;安倍晋三本周会晤特朗普 将强调日本对美国益处&lt;/field&gt; &lt;field name=&quot;content&quot;&gt;日本首相安倍晋三计划2月10日在华盛顿与美国总统特朗普举行会晤时提出加大日本在美国投资的设想&lt;/field&gt; &lt;field name=&quot;reply&quot;&gt;672&lt;/field&gt; &lt;/doc&gt;&lt;/add&gt;&apos; 还可以用其他方式，例如：Data Import Handler，SolrJ等。3，查询查询语法解析器指定Query Parser的方式有两种 用defType参数指定；例如： &amp;q=title:美国&amp;defType=dismax 用Local Parameters语法指定；例如： &amp;q={!dismax df=content}日本 Local Parameters—本地参数本地参数可以为查询字符串添加前缀，目的是为Query Parser提供更多的元数据信息。基本语法是：{!k=v [k=v]...} Note 1：当只有value没有key时，本地参数会默认给一个key，即“type”；这种用法是在指定某个Query Parser。 Note 2：本地参数中有个特殊的key，即“v”，它可以为查询参数指定值。 Note 3：本地参数中可以用$符号引用别的参数的值。 Note 4：下面语句的意思是：定义了一个Filter Query；Query中指定的Query Parser是Function Range Query Parser；该Query Parser是用来解析后面的if()函数的，并限制if函数的返回值的范围最大为1；if()函数的第一个参数是一个gt()函数—比较reply的值大于700的结果。&amp;fq={!frange l=1}if(gt(reply,700),1,0) Common Query Parameters通用查询参数： Standard Query Parser继承自Lucene的Query Parser。Lucene Query Parser的语法形式是：Field name:”Term+操作符”。 细节： 1234不指定Field name时，会用Default Field。Term分两种:1，单个Term。例如hello、你、好、&quot;hello&quot;、&quot;test&quot;等单个单词。2，Phrase短语。它是两头被引号包裹的一组单词。例如：&quot;Hello World&quot;、&quot;你好&quot;、&quot;欢迎回来&quot;。 Solr Standard Query Parser适用的查询参数（Request Handler接收）： DisMax Query Parser是Lucene Query Parser语法的一个子集。适用的查询参数除了上面的通用参数、高亮参数、facet参数之外，还包括以下参数： Extended DisMax Query Parser它除了支持Lucene Query Parser的所有参数、DisMax Query Parser的所有参数外，还支持其他参数。 4，Facet—维度查询Facet可以基于索引文档的某个维度或方面，对查询匹配的索引文档进行分类。可以看做是高级版的Group。 当执行一个Field Facet查询时，它会返回对应的域上，每一个唯一域值的列表以及每一个唯一域值匹配到的文档的总数。当执行一个Query Facet查询时，它会返回对应的子查询上，每一个唯一子查询的列表以及每一个子查询匹配到的文档的总数。 示例：12345678q=*:*&amp; facet=on&amp; //开启Facet查询 fq=reply:[600 TO 800]&amp; //查询并过滤reply域的值在600至800之间的文档 facet.limit=4&amp; //限制内个facet组只返回前4项（term） facet.field=content&amp; //在指定的content域上执行Facet查询，在Facet查询中，会对该域的域值进行分组统计。 facet.field=reply&amp; //指定了reply域，意义同上。 facet.query=content:&quot;日本&quot;&amp; //在指定的子查询：“content:日本”上执行Facet查询，在Facet查询中，会对子查询的结果进行分组统计。 facet.query=content:&quot;美国&quot; //意义同上。 5，Highlighting—高亮查询12345678q=blue fireball in the rain&amp; //查询项 df=sighting_en&amp; //应用的default field。 wt=json&amp; // hl=true&amp; //开启high lighting。 hl.snippets=2&amp; //每个查询到的索引文档，最多返回2个片段 hl.fl=sighting_en&amp; //为sighting_en field应用高亮。 hl.q=blue fireball in the rain light&amp; //在高亮结果中增加一个q查询之外的 “light” term。 fq=shape_s:light //filter query项。 7，Group—分组查询Grouping Results by Field根据Field对查询结果分组。 123456q=*:*&amp; //查询所有项。 sort=popularity asc&amp; //根据popularity排序。 fl=id,type,product,format&amp; //查询结果中返回的field list。 group=true&amp; //对查询结果分组。 group.field=product&amp; //根据product field对查询结果进行分组。 group.main=true //合并每个分组的结果，在response的“docs”下显示。 Grouping by Query根据子查询对结果分组。示例： 12345678q=*:*&amp; //查询所有项。 sort=popularity asc&amp; //根据popularity排序。 fl=id,type,product,format&amp; //查询结果中返回的field list。 group=true&amp; //对查询结果分组。 group.limit=2&amp; //每组返回2个结果。等于-1时表示每组中返回所有结果。 group.query=&quot;The Hunger Games&quot;&amp; //返回每个子查询及结果，并对子查询的结果分组 group.query=games&amp; //同上。 group.query=type:Movies //同上。 Collapsing Query Parser它的作用是：每个field组下面的唯一term值，只返回一个索引文档语法：q=*:*&amp;fq={!collapse field=product}group与它不同的是：group返回的是每个field组下面的所有匹配的索引文档，并且可以通过”group.limit”参数来控制返回1个、多个或者所有个。 8，函数查询语法：functionName(input1,[ ..., inputN])input参数可以是以下任意一种形式： 一个常量值：100、1.45、”hello world” 一个域名称：fieldName、field(fieldName) 一个function：functionName(…) 一个变量：q={!func}min($f1,$f2)&amp;f1=sqrt(popularity)&amp;f2=1 一些特殊的语法： Constant Function的语法是值本身。 Field Function的语法是域的名称被一个名称为“field”的函数包裹。 Parameter Substitution（替换变量）的语法是函数的输入变量是一个$开头的变量，该变量引用自请求URL中查询文本定义的变量。 使用形式：有多种： 对q参数加权；q=solr AND _val_:&quot;recip(ms(date),1,100,100)&quot; 在不同Query Parser中使用；在eDisMax Query Parser中通过bf参数指定function， q=dismax&amp;bf=&quot;ord(popularity)^0.5 recip(rord(price),1,1000,1000)^0.3&quot; 通过本地参数!func来构造一个Function Query；q=solr AND {!func v =&quot;add(l,boostField)&quot;} Function Range Query Parser（简称frange）可以根据函数计算值的范围来过滤索引文档。123q=*:*&amp; fq=&#123;!frange l=10 u=15&#125;product(basePrice,sum(1,$userSalseTax))&amp; //l表示lower最小值，u表示upper最大值。 userSalseTax=0.07 自定义函数： 9，Solr的处理流程搜索流程solr接收到搜索请求后，会根据query type（/select等），找到对应的RequestHandler。如果搜索指定了query parser，则使用指定的query parser对搜索请求进行解析。如果没指定则使用Request Handler配置的query parser。然后是查询索引库的数据，如果有特殊的搜索参数，则会进行特殊的处理；比如fq、sort等。最后使用ResponseWriter将结果返回给用户。 10，SolrCloud搭建zookeeper搭建参考： https://blog.csdn.net/Thinking_one/article/details/89280227 solr配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/bash#配置该环境变量会让solr以cloud的方式运行echo \"export ZK_HOST=192.168.152.128:2181,192.168.152.129:2181,192.168.152.130:2181\" &gt;&gt; /etc/profilesource /etc/profile#下载solr到指定目录wget -q 'http://mirrors.tuna.tsinghua.edu.cn/apache/lucene/solr/7.7.2/solr-7.7.2.tgz' -P /root/#解压出来安装脚本tar xzf /root/solr-7.7.2.tgz solr-7.7.2/bin/install_solr_service.sh '--strip-components=2'#安装solr，安装后会自动运行bash install_solr_service.sh /root/solr-7.7.2.tgz#停止solrservice solr stop#改变solr运行目录所属用户chown -R \"solr:solr\" /opt/solr-7.7.2chown -R \"solr:solr\" /opt/solr#重启solrservice solr start#切换到用户solr的环境下创建collection（SolrCloud模式）或core（standalone）su -c \"/opt/solr/bin/solr create -c restaurants -d /opt/solr-conf/restaurants/ -shards 4 -replicationFactor 2 -n restaurants-conf\" - solrsu -c \"/opt/solr/bin/solr create \\ #运行solr命令 -c ufo \\ #创建collection的名称 -d /opt/solr-conf/ufo/ \\ #collection需要的配置文件的目录，主要是schema.xml和solrconfig.xml文件，该文件需要提前编写。 -shards 4 \\ #指定当前collection的shard数量 -replicationFactor 2 \\ #指定每个shard划分replica的数量 -n ufo-conf\" \\ #指定配置文件目录在zookeeper中的名称，默认-d参数下的所有配置文件会上传到zookeeper中，且以collection的名称（即-c的参数）作为配置文件目录在zookeeper中名称。 - solr #切换到指定用户下执行#手动将一个collection与指定的配置文件目录进行绑定#sh /opt/solr/server/scripts/cloud-scripts/zkcli.sh -zkhost 192.168.152.128 -cmd linkconfig -collection restaurants -confname restaurants-conf#sh /opt/solr/server/scripts/cloud-scripts/zkcli.sh -zkhost 192.168.152.128 -cmd linkconfig -collection ufo -confname ufo-conf#手动创建core#curl 'http://localhost:8983/solr/admin/cores?action=CREATE&amp;name=restaurants&amp;collection=restaurants'#curl \"http://localhost:8983/solr/admin/cores?action=CREATE&amp;name=ufo&amp;collection=ufo\"#删除collection#su -c \"/opt/solr/bin/solr delete -c ufo -deleteConfig true\" - solr SolrCloud的核心概念1，物理概念 节点（Node）：运行solr服务的单个实例。 核心（Core）：是用来承载Document的，可以理解为多个Document的集合；一个节点可以有多个核心。 副本（Replica）：是某一个具体的核心的副本，可以存在于同一个节点上，也可以存在于集群中的其他节点上。 2，逻辑概念 分片（Shard）：一组的核心的副本。处理向核心发送的添加、同步、查询等任何请求。 集合（Collection）：集群中的一组分片。 集群（Cluster）：在solr实例和solr cloud中配置的多个活动的集合。 索引文档路由创建索引时的两种使用方式： 指定router.name参数来设置文档路由。 默认使用自动路由（compositeID），可以创建并发送一个document id带有前缀的索引文档；形式是：“前缀!真实的document id”，例如：“IBM!123”（也叫自定义散列）。搜索时：使用route参数指定前缀字符。 它的作用是，通过计算前缀的hash值，来确定该索引文档发送到那个shard，这样前缀相同的文档就会发送到相同的shard，从而也可以实现group和join查询。查询时可以指定Shard，避免了在所有Shard之间查询的网络延迟，提高查询效率。 确定Shard LeaderShard Leader负责接收更新请求，并将这些请求协调分配给各个副本。 SolrCloud分布式索引solrCloud中分布式索引的总体步骤是：能够将文档发送到集群中的各个节点，并且能够在正确的分片中被索引。 1，将文档分配给分片索引文档时需要将文档分配给其中一个分片，并且一个文档只能分配给每个集群中的一个分片。Solr使用文档路由器组件来确定文档被分配到那个分片。SolrCloud有两种路由策略： compositeID（默认）。我们知道，创建Collection时必须指定Shard的数量。确定Shard的数量后，Solr会给每个Shard分配一个32位的散列值。例如，创建Collection时，指定了2个Shard；则分配给shard1的散列区间是80000000-ffffffff，分配给shard2的散列区间是0-7fffffff（16进制值）。Solr会在每个分片之间均分这个32位的散列值。compositeID路由器会计算出文档中唯一ID字段的数值散列值；然后Solr会将文档分配给散列值区间包含文档的散列值的Shared上，用的算法是MurmurHash算法。 隐式路由。 2，添加文档的步骤： 使用solrJ的CloudSolrServer发送更新请求。CloudSolrServer连接zookeeper以获取集群的当前状态；也知道哪些节点是分片代表；它也提供了基本的负载均衡和客户端重试逻辑。 将文档分配给正确的分片。CloudSolrServer需要使用文档路由进程来确定将文档发送到哪个分片上。 代表分配版本ID号。在将文档发送给副本之前，分片代表会本地索引文档；代表会给每一个新文档分配一个版本号。 将请求转发给副本。 确认写操作成功。 3，节点恢复SolrCloud能妥善的处理脱机节点，他提供了两种基本的恢复方案： 对等同步 快照复制这也表示可以在任何时间内将副本添加到集群中，因为它能自己从分片代表获取完整的索引。 SolrCloud分布式查询查询集合中的所有分片并创建统一结果集的过程称为分布式查询。 1，查询流程分布式查询过程中，solr需要将各个分片的结果汇总，然后将其合并为单个结果，最后响应给客户端。 客户端将查询请求发送到任何节点。 查询控制器接受请求。 查询。查询控制器会给每个分片发送一个非分布式查询；被发送给每个分片的查询式只请求id和score字段，不会过早读取所有存储字段，只有等到最后正确的文档被确认，才读取需要的文档中的存储字段。 读取字段。当上一步的查询确认了匹配的文档，查询控制器才会把第二步查询发送给节点的子集，以便得到满足请求的其他字段；如果请求需要的仅仅是ID字段，则不会发送第二步查询；另外，只有包含需要返回的文档的分片，才会接收第二步查询。 2，查询的局限性 按权重对文档排序时，会出现偏差。因为计算权重只用到了本地索引中的词频。 连接（join）在分布式环境下不起作用。除非使用自定义散列。 为了使用分组（group）功能，需要使用自定义散列去分配那些需要被分到同组的文档。","tags":[]},{"title":"lucene的使用","date":"2018-12-04T04:00:14.000Z","path":"2018/12/04/lucene-usage/","text":"1，Lucene介绍Lucene是一个开源的全文检索工具包，他的主要作用有两个： 索引文档 搜索文档 2，分词器（Analyzer） Analyzer是Lucene中比较重要的概念，在“索引文档”和“搜索文档”的时候都会用到Analyzer。 Analyzer类是一个抽象类，切分词的实现主要由子类实现，所以对不同的语言规则，要有不同的分词器。Analyzer内部主要通过TokenStream类实现。Tokenizer类和TokenFilter类是TokenStream的两个子类。Tokenizer负责处理单个字符组成的字符流，读取Reader中的数据，处理后转成词汇单元。TokenFilter完成文本过滤器的功能。 3，索引 索引过程就是把文档变成索引这种数据结构的过程，也可以说是创建倒排索引表的过程。 具体过程包括：创建Document、定义FieldType、设置Stored、设置Tokenized等。 4，查询Lucene的查询方式有两种： 使用查询表达式 可参考：http://www.cnblogs.com/xing901022/p/4974977.html 使用Query API 具体可查询Lucene API或相关书籍 5，Lucene中一些深入的概念1，倒排索引（Inverted Index）倒排索引是一种索引方法，也是一种数据结构；被用来存储，在全文索引搜索下，某个单词在一个文档或一组文档中的存储位置的映射。 正排索引是以document id为key，其映射的内容记录着单词出现的次数、位置。如果要搜索某个单词必须要扫描所有的document，才能正确的找到所有包含此单词的document，然后经过处理呈现给用户。倒排索引是以每个单词为key，其映射的内容记录着所有包含此单词的document id的列表。如果要搜索某个单词只需要扫描索引记录，就能直接找到所有正确的document呈现给用户。 2，向量空间模型（Vector Space Model）待完成","tags":[]},{"title":"linux中ssh的使用","date":"2018-11-23T13:23:21.000Z","path":"2018/11/23/linux-ssh-usage/","text":"实现linux服务器的免密码登录 A是客户端，B是要登录的服务端。 A的操作 用ssh-keygen生成A的秘钥对。 用ssh-copy-id [B的ip]命令把A的公钥发送到B。 A可以直接用ssh 用户名@[B的ip]登录","tags":[]},{"title":"关于数据库中的事务和锁","date":"2018-11-13T13:40:09.000Z","path":"2018/11/13/sql-transaction-and-lock/","text":"事务事务是由一组SQL查询或更新组成的独立的操作单元。事务中的SQL语句要么全部执行成功，要么全部执行失败。 事务的特性 原子性：一个事务必须是一组不可分割的最小操作单元。事务中的全部操作，要么全部提交成功，要么全部失败回滚。 一致性：事务中的数据总是从一个一致性的状态转换到另一个一致性的状态。 隔离性：一般情况下，一个事务的操作在提交成功之前，其他事务是看不到的。特殊情况下，不同的“隔离级别”，会有不同的隔离性。 持久性：事务提交之后，其所做的修改会永久保存到数据库中。 事务的隔离级别数据库事务有不同的隔离级别，不同的隔离级别对锁的使用是不同的，锁的应用最终导致不同事务的隔离级别。 read uncommitted（未提交读）：会产生脏读。 A事务开启，做了查询。 B事务开启，更新了一条数据，没有提交。 A事务查询，会得到B事务更新的数据，此时A事务读取到了脏数据，这种情况叫做脏读。 read committed（提交读）：会产生不可重复读。 A事务开启，做了查询。 B事务开启，更新了一条数据，没有提交。 A事务查询，得到的还是A事务原来的数据，此时解决了A事务的脏读。 B事务提交。 A事务查询，得到的是B事务更新并提交后的数据；此时A事务读取到的数据和前几次A事务读取的数据不一致，这个种情况叫做不可重复读。 repeatable read（可重复读）：会产生幻读。 A事务开启，做了查询。 B事务开启，更新了一条数据，并提交。 A事务查询，得到的还是A事务此前的数据，此时解决了A事务的不可重复读。 （InnoDB不会出现下面的情况） B重新事务开启，新增了一条数据，并提交。 A事务查询，会得到B事务新增的一行数据，这行数据叫做幻行，这种情况叫做幻读。InnoDB通过多版本并发控制（MVCC）解决了幻读。 serializable A事务开启，做了查询。 B事务开启，新增了一条数据，此时会插入失败；解决了幻读问题 锁使用锁是为了支持对共享数据的并发访问，保证数据的一致性和完整性。 锁的分类 表锁 行级锁。又可分为： 共享锁（Shared lock），也成读锁。S锁与S锁之间兼容；S锁与X锁之间互斥。 排它锁（Exclusive lock），也称写锁。X锁与X锁/S锁之间都互斥。 锁的应用 read uncommitted；“读”不会加任何锁；“写”会加X锁，并到事务结束之后释放。 上例中的A事务还能读取B事务中加了X锁的数据，是因为A事务没有加任何锁，不会被X锁排斥。 read committed；“读”不会加任何锁，使用MVCC机制读取了源数据的镜像版本；“写”会加X锁。 - repeatable read；“读”不会加任何锁，使用MVCC机制读取了源数据的镜像版本；“写”会加X锁。 - serializable；读加S锁，写加X锁 多版本并发控制（MVCC）只在read committed和repeatable read隔离级别下工作。","tags":[]},{"title":"关于SQL中的索引","date":"2018-11-13T12:23:10.000Z","path":"2018/11/13/sql-index-summary/","text":"索引是存储引擎用于快速查找数据记录的一种数据结构。可以比作为书的“目录”。 索引分类按照数据结构 B+Tree索引 Hash索引 按照定义 普通索引；列值允许空值和重复值。 唯一索引；列值不允许有重复值。 单列索引；由单个列组成的索引。 多列索引；由多个列组成的索引，以最左边的列为准，查询条件中必须出现最左边的列。 全文索引；索引的列上支持全文查找。 空间索引；对空间数据类型的列建立的索引。只能用在MyISAM引擎中。 按照存储方式一些概念：叶子结点也叫终端结点，就是没有子结点的结点主键索引是系统创建的索引；二级索引是自定义的索引 聚簇索引 表数据是和索引数据（主键）是存储在一起的。 主键索引的叶子结点存储了行数据，并包含主键值。 二级索引的叶子结点存储了行的主键值。 InnoDB的索引存储机制，使用的是聚簇索引。 InnoDB的表数据文件就是一个由B+Tree数据结构组成的一个索引文件。 InnoDB要求表必须有主键（MyISAM可以没有）;如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键;如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形 非聚簇索引 表数据和索引数据是分开存储的。 主键索引的叶子结点存储的是行数据的地址。 二级索引同上 MyISAM的索引存储机制，使用的是非聚簇索引。也是由B+Tree数据结构组成的索引文件。创建原则 索引并非越多越好 数据量小的表不要建索引 在重复值多的列上不要建索引。（位图索引？） 对于经常查询的字段创建索引 数据唯一时创建唯一索引 在频繁进行order by和group by的列上建立索引 使用注意点在where语句中： 使用多列索引时，注意最左匹配原则。MySQL在匹配索引时，会一直向右匹配，只要匹配到索引，查询就会走索引；直到遇到范围查询（&lt;，&gt;，between，like），就停止匹配；所以在创建索引时，用到范围查询的列要放在最右边。 使用多列索引时，MySQL的查询优化器会优化索引的使用顺序。in和=可以乱序使用多列索引。 索引列用于函数、算术运算时，不会使用索引；在select语句中会使用索引。 对索引列进行is null、is not null判断，不会使用索引。 or操作符，不会使用索引。 查询参数的数据类型和索引列的数据类型不一致时，不会走索引。 负向查询（not , not in, not like, &lt;&gt;, != ,!&gt;,!&lt; ）不会使用索引。 like操作符的全模糊’%a%’和左模糊’%a’，不会使用索引。 使用多列索引时，索引的第一列必须出现在查询条件中，否则不会走索引。 优点和缺点 优点 缺点 唯一索引可以保证每行数据的唯一性 创建和更新索引消耗时间 可以加快数据的检索速度 占用存储空间 加快表与表之间的连接速度 表中数据变动会引起索引变动 减少查询中排序和分组的时间 一些深入的问题为什么查询快？因为各个数据页中的记录并没有规律，我们并不知道搜索条件匹配哪些页中的记录，所以不得不依次遍历。而建了索引后，索引目录就遵循以下规则： 下一个数据页的主键值必须大于上一个页中的主键值 给所有的页建立一个目录项，页的用户记录中的最小的主键值作为key，页号作为value。所以在页面内也可以通过二分法实现快速定位记录。 为什么删除和更新操作慢？ B+树是一颗平衡树，如果我们对这颗树增删改的话，那肯定会破坏它的原有结构。 要维持平衡树，就必须做额外的工作。正因为这些额外的工作开销，导致索引会降低增删改的速度","tags":[]},{"title":"spring boot自动配置的使用及原理分析","date":"2018-11-05T03:06:51.000Z","path":"2018/11/05/springboot-autoconfig-analyse/","text":"spring boot提供的自动配置，可以简化项目的配置，封装成starter后可以降低项目之间的依赖。 spring boot使用自动配置的方式有两种： 1.在spring.factories文件中增加配置类 + @Configuration +@ConditionalOnClass/@ConditionalOnMissingBean + @ConfigurationProperties + @EnableConfigurationProperties + @EnableAutoConfiguration。这种方法可以在存在或者不存在一些类时，直接加载配置。 原理分析：12345678910111213141516171819202122232425/***应用的启动类*/@Configuration@EnableAutoConfiguration@ComponentScan //默认扫描所在包下面的class。@EnableHelloStarter //启用自定义的starter模块。相当于导入Auto Configuration类；其中的package name尽量不要和调用者的package name相同。@RestControllerpublic class App &#123; @Autowired HelloServiceExtendProperties helloServiceExtendProperties; @Autowired private HelloService helloService; @RequestMapping(\"/\") public String index() &#123; helloService.setMsg(helloServiceExtendProperties.getMsg()); return helloService.sayHello(); &#125; public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125;&#125; 上面的@EnableAutoConfiguration是自动配置的关键，它会调用AutoConfigurationImportSelector的selectImports方法，该方法会获取所有的配置类。最终spring会调用配置类的后置处理器-ConfigurationClassPostProcessor，将配置类注册到spring容器中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class AutoConfigurationImportSelector implements DeferredImportSelector, BeanClassLoaderAware, ResourceLoaderAware, BeanFactoryAware, EnvironmentAware, Ordered &#123; private static final String[] NO_IMPORTS = &#123;&#125;; private static final Log logger = LogFactory .getLog(AutoConfigurationImportSelector.class); private ConfigurableListableBeanFactory beanFactory; private Environment environment; private ClassLoader beanClassLoader; private ResourceLoader resourceLoader; @Override public String[] selectImports(AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return NO_IMPORTS; &#125; try &#123; AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader .loadMetadata(this.beanClassLoader); AnnotationAttributes attributes = getAttributes(annotationMetadata); List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); ...... return configurations.toArray(new String[configurations.size()]); &#125; catch (IOException ex) &#123; throw new IllegalStateException(ex); &#125; &#125; ...... /** * 获取@EnableAutoConfiguration注解的属性。 */ protected AnnotationAttributes getAttributes(AnnotationMetadata metadata) &#123; String name = getAnnotationClass().getName(); AnnotationAttributes attributes = AnnotationAttributes .fromMap(metadata.getAnnotationAttributes(name, true)); Assert.notNull(attributes, \"No auto-configuration attributes found. Is \" + metadata.getClassName() + \" annotated with \" + ClassUtils.getShortName(name) + \"?\"); return attributes; &#125; ...... /** * 获取META-INF/spring.factories中的所有配置类 */ protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123; List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, \"No auto configuration classes found in META-INF/spring.factories. If you \" + \"are using a custom packaging, make sure that file is correct.\"); return configurations; &#125; ......&#125; 2.定义Enable*注解 + @Import + @Configuration +@ConditionalOnClass/@ConditionalOnMissingBean + @ConfigurationProperties + @EnableConfigurationProperties + @EnableAutoConfiguration。这种方法可以有选择性的加载配置。 原理分析：12345678@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@Import(HelloServiceAutoconfiguration.class)public @interface EnableHelloStarter &#123;&#125; Enable*注解的@Import会用三种方式将配置类注册到spring容器中，方式如下 12345678910111213@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Import &#123; /** *支持下面三种注册Bean的方式： * &#123;@link Configuration&#125;, &#123;@link ImportSelector&#125;, &#123;@link ImportBeanDefinitionRegistrar&#125; * or regular component classes to import. */ Class&lt;?&gt;[] value();&#125; 参考：https://blog.csdn.net/xichenguan/article/details/73478873","tags":[]},{"title":"java并发-线程池和Executor","date":"2018-10-13T03:48:27.000Z","path":"2018/10/13/java-concurrent-five/","text":"线程池的使用和实现原理： Excetor框架有两个关键类实现了ExecutorService接口：继承关系图如下： 虚线是implements，实线是extends。 1，ThreadPoolExecutor它是线程池的实现类，主要有下列四个参数构成： corePoolSize：核心线程池的大小 maximumPoolSize：最大线程池的大小 BlockingQueue： 用来保存等待执行的任务的阻塞队列，可以选择以下几个阻塞队列 ArrayBlockingQueue：是一个基于数组结构的有界阻塞队列，此队列按 FIFO（先进先出）原则对元素进行排序。 LinkedBlockingQueue：一个基于链表结构的阻塞队列，此队列按FIFO （先进先出） 排序元素，吞吐量通常要高于ArrayBlockingQueue。静态工厂方法Executors.newFixedThreadPool()使用了这个队列。 SynchronousQueue：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQueue，静态工厂方法Executors.newCachedThreadPool使用了这个队列。 PriorityBlockingQueue：一个具有优先级得无限阻塞队列。 RejectedExecutionHandler：当ThreadPoolExecutor已经关闭或ThreadPoolExecutor已经饱和时（达到最大线程池大小且工作队列已满），需要一种策略处理提交的新任务。它也是execute()方法将要调用的Handler 还有三个不常用的参数： keepAliveTime：线程池的工作线程空闲后，线程保持存活的时间。如果任务很多，且每个任务执行的时间很短，可以调大这个参数，提高线程利用率。 TimeUnit：线程活动保持时间的单位。 ThreadFactory：主要用来创建线程。 ThreadPoolExecutor通常由工厂类Executors来创建，Executors可以创建三种ThreadPoolExecutor： 1，FixedThreadPool它可以创建可重用的固定线程数的线程池；适用于需要限制当前线程数的场景，适用与负载比较重的服务器。 12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 2，SingleThreadExecutor它可以创建单个线程的线程池；适用于需要顺序执行各个任务的应用场景。 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 3，CacheThreadPool它可以创建大小无边界的线程池；适用于执行短期异步任务的应用，或者负载较轻的服务器。 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 2，ScheduledThreadPoolExecutor它继承自ThreadPoolExecutor，可以实现延时或者定期执行任务。通常也是由Executors创建，Executors可以创建两种ScheduledThreadPoolExecutor： ScheduledThreadPoolExecutor：包含多个线程。 SingleThreadScheduledExecutor：只包含一个线程。","tags":[]},{"title":"java并发-java并发容器和框架","date":"2018-10-10T13:56:31.000Z","path":"2018/10/10/java-concurrent-four/","text":"并发容器和框架 1. ConcurrentHashMap的实现原理和使用HashMap在并发操作下引起死循环的原因：在执行put()方法时会使Entry链表形成环形数据结构，一旦形成环形链表，Entry的next节点永远不为空，就会产生死循环获取Entry。 结构（java7）ConcurrentHashMap由Segment数组和HashEntry数组结构组成。Segment是一种可重入锁（ReentrantLock）;HashEntry用于存储键值对数据。一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素。每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组进行修改时，必须获得与它对应的Segment锁。 （java8）多了一种数据结构-红黑树，当链表长度大于8就会被转化为红黑树。 初始化（java7）initialCapacity-初始化ConcurrentHashMap容量为16；loadFactor-负载因子为0.75；ConcurrencyLevel-并发级别为16，理论上，如果线程的操作分布在不同的Segment上，，则最多支持16个线程并发写。 （java8） get方法（java7）经过一次hash算法定位到Segment，再经过一次hash算法定位到HashEntry。基于volatile的内存语义，所以get操作没有加锁。 （java8）计算hash值，根据hash值找到数组对应位置；根据该位置处的节点性质进行相应查找：如果该位置为null，就返回null，如果找到对应值，就返回该节点的值，如果是正在扩容，或者是红黑树，在调用find方法，如果是链表，则进行遍历查找。 put方法（java7）put方法首先定位到Segment，然后在Segment里进行插入操作。插入操作有两个步骤：一是判断是否需要对Segment里的HashEntry数组进行扩容；二是定位添加元素的位置，然后将其放到HashEntry数组里。 （java8）1，初始化数组；2，找到hash值对应的数组下表，然后用CAS尝试放入新值；3，数据迁移；4，遍历及放入链表；5，调用红黑树并插入；6，链表转换为红黑树 2. ConcurrentLinkedQueue3. java中的阻塞队列4. Fork/Join框架","tags":[]},{"title":"java并发-java中的锁","date":"2018-09-19T12:32:25.000Z","path":"2018/09/19/java-concurrent-three/","text":"java中的锁围绕两个方面：“使用”，“实现”。 1. Lock接口Lock接口提供了synchronized所不具备的特性： 特性 描述 尝试非阻塞地获取锁 当线程尝试获取锁，如果这一时刻锁没有被其他线程获取到，则成功获取到锁。 能被中断的获取锁 与synchronized不同，获取到锁的线程能够响应中断，当获取到锁的线程被中断，中断异常会被抛出，同时锁会被释放。 超时获取锁 在指定的时间内获取锁，如果时间到了仍无法获取锁，则返回。 2. 队列同步器（QAS）AbstractQueuedSynchronizer是用来构建锁或其他同步组件的基础框架，它使用一个int成员变量表示同步状态；通过内置的FIFO队列来完成资源获取线程的排队工作。 &emsp;&emsp;使用自定义同步组件（CustomLock）需要聚合一个AQS的子类（SpecificSynchronizer）。SpecificSynchronizer可重写的方法： 方法名称 描述 protected boolean tryAcquire (int arg) 独占式获取同步状态。实现该方法需要查询当前状态并判断同步状态是否符合预期，然后再进行 CAS 设置同步状态。 protected boolean tryRelease (int arg) 独占式释放同步状态。等待获取同步状态的线程将有机会获取同步状态。 protected int tryAcquireShared (int arg) 共享式获取同步状态。返回大于等于0的值，表示获取成功，反之获取失败。 protected boolean tryReleaseShared (int arg) 共享式释放同步状态。 protected boolean isHeldExclusively() 当前同步器是否在独占模式卜被线程占用，一般该方法表示是否被当前线程所独占。 CustomLock可调用的SpecificSynchronizer提供的部分模板方法如下： 方法名称 描述 void acquire (int arg) 独占式获取同步状态。如果当前线程获取同步状态成功，则由该方法返回；否则，将会进入同步队列等待，该方法将会调用重写的tryAcquire(int arg)方法。 void acquireInterruptibly (int arg) 与acquire (int arg)相同，但是该方法响应中断，当前线程未获取到同步状态而进人同步队列中，如果当前线程被中断，则该方法会抛出InterruptedException并返回。 boolean tryAcquireNanos (int arg, long nanos) 在acquireInterruptibly (int arg)的基础上增加了超时限制，如果当前线程在超时时间内没有获取到同步状态，那么将会返回false，如果获取到了返回true。 void acquireShared(int arg) 共享式的获取同步状态，如果当前线程未获取到同步状态，将会进入同步队列等待，与独占式获取的主要区别是在同一时刻可以有多个线程获取到同步状态。 void acquireSharedInterruptibly(int arg) 与acquireShared(int arg)相同，该方法响应中断。 boolean tryAcquireSharedNanos(int arg, long nanos) 与acquireSharedInterruptibly(int arg)基础上增加了超时等待限制。 boolean release(int arg) 独占式释放同步状态，该方法会在释放同步状态后，将同步队列中第一个节点包含的线程唤醒。 boolean releaseShared(int arg) 共享式的释放同步状态。 Collection getQueuedThreads() 获取等待在同步队列上的线程集合。 &emsp;&emsp;实现：&emsp;&emsp;独占式获取同步状态的关键点 AQS依赖内部的同步队列（一个FIFO的双向队列）来完成同步状态的管理。同步队列中节点（Node）用来保存获取同步状态失败的线程引用、等待状态以及前驱节点和后继节点。 通过调用AQS的acquire(int arg)方法可以获取同步状态。 首先调用SpecificSynchronizer实现的tryAcquire(int arg)，该方法保证线程安全的获取同步状态。 如果获取同步状态失败，通过调用addWaiter(Node node)方法将该节点加入到同步队列的尾部。主要是有一段快速尝试在尾部添加节点的操作。 在enq(final Node node)方法中，主要是通过CAS将节点正确地设置成尾节点。 节点加入同步队列后，调用acquireQueued(final Node node, int arg)方法进入一个自旋的过程。每个节点（或者说每个线程）都在自省的观察；当条件满足，获取到了同步状态，就会从自旋过程中退出；否则依旧停留在自旋过程中。 acquireQueued(final Node node, int arg)的主要操作有两个：一是判断是否当前节点的前驱节点是head节点，并且成功获取到了同步状态；则将当前节点设置为head节点，并将前驱节点设置为null。最后是返回并退出自旋，继续执行线程任务。二是根据当前节点的前驱节点的waitStatus是否是SIGNAL来决定是否park当前节点中的线程。这里有一段将前驱节点的waitStatus设置为SIGNAL的操作，所以同步队列中的任意节点的前驱节点的waitStatus为SIGNAL，则任意节点中的线程就会park。 release(int arg)的作用是调用tryRelease(int arg)将state设置为0，如果成功则唤醒（unpark）在队列自旋中park的，并且是head节点的后继节点中的线程。 参考测试代码：https://github.com/legend9207/concurrent_code/blob/master/src/com/roocon/thread/ta4/Main.java &emsp;&emsp;共享式获取同步状态的关键点待完成 3. 重入锁&emsp;&emsp;实现：包含3个要素： 原子状态。原子状态使用CAS操作来存储当前锁的状态，进而判断锁是否被别的线程所持有。 等待队列。所有没有请求到锁的线程，会进入等待队列进行等待；带有线程释放锁后，jvm就能从等待队列中唤醒一个线程，继续工作。 阻塞原语park()和unpark()。用来挂起和恢复线程。 nonfairTryAcquire增加了再次获取同步状态的逻辑：判断当前线程是否为获取锁的线程；如果是，则将同步状态的值增加并返回true，表示获取同步状态成功。 tryAcquire增加了同步队列中的当前节点是否有前驱节点的判断，如果有，则表示有线程比当前线程更早的请求锁。所以当前还要继续等待，不能成功获取锁。 tryRelease会判断，在经过前面n-1次的tryRelease后；判断此次的同步状态是否为0，如果是，则将占有线程设为null，并返回true，表示释放成功。 &emsp;&emsp;公平性大多数情况下，锁的申请都是非公平的。例如，线程1请求了锁A，线程2也请求了锁A，当锁A可用时，是线程1还是线程2获取到锁是不确定的；这种情况下会产生饥饿现象。ReentrantLock支持对获取锁时公平性的设置。 4. 读写锁读写锁允许多个线程同时访问。 &emsp;&emsp;实现： 读写锁的同步器需要在同步状态（变量-state）上维护多个读线程和一个写线程的状态。这样就需要“按位切割使用”这个变量：高16位表示读，低16位表示写。 源码解读 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283/**ReentrantReadWriteLock中的部分源码，主要分析“写锁”。@see java.util.concurrent.locks.ReentrantReadWriteLock*/static final int SHARED_SHIFT = 16;static final int SHARED_UNIT = (1 &lt;&lt; SHARED_SHIFT);static final int MAX_COUNT = (1 &lt;&lt; SHARED_SHIFT) - 1;/**下面的公式转成二进制的运算：= 0000 0000 0000 0001 0000 0000 0000 0000 - 1= 1*2^16 - 1= 1*2^15= 0000 0000 0000 0000 1111 1111 1111 1111意思是取低位的最大值。*/static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1;/** Returns the number of shared holds represented in count */static int sharedCount(int c) &#123; return c &gt;&gt;&gt; SHARED_SHIFT; &#125;/** Returns the number of exclusive holds represented in count 返回持有写锁的线程的数量如果c = 0；则：0000 0000 0000 0000 0000 0000 0000 0000 &amp; 0000 0000 0000 0000 1111 1111 1111 1111 = 0000 0000 0000 0000 0000 0000 0000 0000如果c = 1；则：0000 0000 0000 0000 0000 0000 0000 0001 &amp; 0000 0000 0000 0000 1111 1111 1111 1111 = 0000 0000 0000 0000 0000 0000 0000 0001可以得知，其意思是计算低位的值。 */static int exclusiveCount(int c) &#123; return c &amp; EXCLUSIVE_MASK; &#125;/**该方法最终被ReentrantReadWriteLock.WriteLock调用。@see java.util.concurrent.locks.ReentrantReadWriteLock.WriteLock#lock*/protected final boolean tryAcquire(int acquires) &#123; /* * Walkthrough: * 1. If read count nonzero or write count nonzero * and owner is a different thread, fail. * 2. If count would saturate, fail. (This can only * happen if count is already nonzero.) * 3. Otherwise, this thread is eligible for lock if * it is either a reentrant acquire or * queue policy allows it. If so, update state * and set owner. */ Thread current = Thread.currentThread(); int c = getState(); //获取持有写锁的线程数量。 int w = exclusiveCount(c); if (c != 0) &#123; // (Note: if c != 0 and w == 0 then shared count != 0) /** 在取到写锁线程的数目后，首先判断是否已经有线程持有了锁，如果已经有线程持有了锁(c!=0)， 则看当前持有写锁的线程的数目，如果写线程数（w）为0（那么读线程数就不为0，因为上面的c和此处的w都是取的state，而读锁和写锁都依赖state。） 或者独占锁线程（持有锁的线程）不是当前线程就返回失败； 如果写入锁的数量（其实是重入数）大于65535就抛出一个Error异常。 */ if (w == 0 || current != getExclusiveOwnerThread()) return false; if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(\"Maximum lock count exceeded\"); // Reentrant acquire setState(c + acquires); return true; &#125; /** writerShouldBlock如果返回true，则表示当前线程是在运行在公平锁下，并且有前驱节点，需要阻塞； 如果返回false，则表示当前线程没有前驱节点或者运行在非公平锁下，不需要阻塞。 @see java.util.concurrent.locks.ReentrantReadWriteLock.FairSync#writerShouldBlock compareAndSetState如果返回true，则表示成功获取到锁； */ if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true;&#125; 源码解读参考：https://blog.csdn.net/MeituanTech/article/details/84138163","tags":[]},{"title":"java并发-并发编程基础","date":"2018-09-13T13:29:13.000Z","path":"2018/09/13/java-concurrent-two/","text":"java并发编程基础1. 线程简介线程都拥有各自的计数器、堆栈和局部变量等属性，并且能够访问共享的内存变量。 &emsp;&emsp;线程状态：NEW：初始状态，线程被构建，但是还没有调用start()方法。RUNNABLE：运行状态，java将操作系统中的“就绪”和“运行”两种状态统称为“运行中”。此时处于Thread.start()方法调用之后。处于WAITING，TIME_WAITING状态的线程在调用Object.notify()，Object.notifyAll()，LockSupport.unpark(Thread)方法之后，也会进入RUNNABLE状态。BLOCKED：阻塞状态，表示线程阻塞于锁。此时线程等待进入synchronized代码块或方法。获取到锁后进入RUNNABLE状态。WAITING：等待状态，进入该状态表示当前线程需要其他线程做出一些特定的动作（通知或中断），此时处于Object.wait()，Object.join()或LockSupport.park()方法调用之后。TIME_WAITING：超时等待状态，它与WAITING不同，它可以在指定的时间自行返回。此时处于Thread.sleep(long)，Object.wait(long)，Thread.join(long)，LockSupport.parkNanos()，LockSupport.parkUntil()方法调用之后。TERMINATED：终止状态，表示线程已经执行完毕。 2. 启动和终止线程&emsp;&emsp;理解中断 中断好比其他线程对该线程打了个招呼，其他线程通过调用该线程的interrupt()方法对其进行中断操作（设置了中断标识位）。 许多声明抛出InterruptedException的方法（例如Thread.sleep(long millis)方法）这些方法在抛出InterruptedException之前，JVM会先将该线程的中断标识位清除，然后抛出InterruptedException，此时调用线程对象的isInterrupted()方法依旧会返回false。 3. 线程间通信&emsp;&emsp;等待通知范式 等待方遵循的原则： “线程”获取对象的锁。wait的是“线程”对象。 如果条件不满足则调用对象的wait()方法，被通知后仍要检查条件。 条件满足则执行对应的逻辑。代码：123456synchronized(对象) &#123; while(条件不满足) &#123; 对象.wait(); &#125; 处理对应的逻辑&#125; 当“对象锁”是Thread对象时，此时wait的是获取“对象锁”的线程，而不是Thread对象。 通知方遵循的原则： “线程”获取对象的锁。 改变条件。 通知所有等待在对象上的线程。代码：1234synchronized(对象)&#123; 改变条件 对象.notifyAll();&#125; 当main线程调用t.join时候，main线程会获得线程对象t的锁（wait意味着拿到该对象的锁)，调用该对象的wait(等待时间)，直到该对象唤醒main线程，比如退出后。这就意味着main线程调用t.join时，必须能够拿到线程t对象的锁。 Thread.join的原理解释：1234//场景：main() &#123; thread.join();&#125; 1234567891011//解释：//此时的“对象锁”是thread对象；相当于范式中的“synchronized(对象)”。public final synchronized void join() throws InterruptedException &#123; // 条件不满足，继续等待 while (isAlive()) &#123; //thread对象调用自身的wait方法。 //相当于范式中“对象.wait()”。所以此时等待的是“main线程对象”。 wait(0); &#125; // 条件符合，方法返回&#125; 4. 线程应用实例待完成","tags":[]},{"title":"java并发-java内存模型","date":"2018-09-03T13:33:54.000Z","path":"2018/09/03/java-concurrent-one/","text":"java内存模型1. 内存模型JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每一个线程都有一个私有的本地内存，本地内存存储了该线程用来读/写共享变量的副本。JMM通过控制主内存与每个线程的本地内存之间的交互，来为java程序提供内存可见性的保证。 2. volatile&emsp;&emsp;特性 可见性。对一个volatile变量的读，总是能看到对这个volatile变量最后的写入。 原子性。对于单个volatile变量的读写具有原子性，但类似于volatile++的复合操作不具有原子性。《java高并发编程详解》把“禁止重排序（内存屏障）”也当做是volatile的语义，《java并发编程的艺术》把“禁止重排序（内存屏障）”当做是volatile的语义的实现 &emsp;&emsp;写-读的内存语义 当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存。 当读一个volatile变量时，JMM会把该行程对应的本地内存的值置为无效，线程接下来将从主内存中读取共享变量。 &emsp;&emsp;内存语义实现原理为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。下面是基于保守策略的JMM内存屏障插入策略： 在每一个volatile写操作的前面插入一个StoreStore屏障。 在每一个volatile写操作的后面插入一个StoreLoad屏障。 在每一个volatile读操作的后面插入一个LoadLoad屏障。 在每一个volatile读操作的后面插入一个LoadStore屏障。 3. synchronized&emsp;&emsp;特性锁可以让临界区互斥执行。 &emsp;&emsp;释放-获取的内存语义 当线程释放锁时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存中。 当线程获取锁时，JMM会把该线程对应的本地内存置为无效。从而使线程中的代码必须从主内存中读取共享变量。 &emsp;&emsp;内存语义实现原理 利用volatile变量的读-写所具有的的内存语义。 利用CAS所附带的volatile读和volatile写的内存语义。 4. final 在构造函数内对一个final变量的赋值（写），与随后把“这个被构造的对象的引用”赋值给“一个引用变量”，这两个操作之间不能重排序。 初次读一个包含“final变量的对象”的引用，与随后初次读“这个final变量”，这两个操作之间不能重排序。 注意点：final只能在两种情况下初始化：1，定义时；2，构造函数内。final引用不能从构造函数内“溢出” 作用 禁止继承重写，防止破坏类内部的线程安全性。 禁止改变变量的值，保证在多线程环境下的数据安全。","tags":[]}]